{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Exercises Week 9"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**In this sequence of exercises we will use [networkx](https://networkx.github.io/) to simplify some graph function and check the correctness of some implmentation** \n", "\n", "**Note that Networkx works well on small graphs / networks, for larger networks it is better to use libraries like [Graph-tool](https://graph-tool.skewed.de/) which are based on C implementations** \n", "\n", "If you don't have Networkx installed please run the code below."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import sys\n", "!{sys.executable} -m pip install networkx"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Exercise 1 - play with graphs\n", "\n", "Networkx allows some simple operation in graphs, for instance one can load a graph and plot it. However the layout will change every-time randomly as the nodes will be distributed using some randomized algorithm. \n", "\n", "```python\n", "G = nx.karate_club_graph()\n", "nx.draw(G, with_labels=True)\n", "plt.show()\n", "```\n", "\n", "If you are interested to know from where this graph comes from, look at [this](https://en.wikipedia.org/wiki/Zachary%27s_karate_club) page."]}, {"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [], "source": ["%matplotlib inline\n", "\n", "import networkx as nx\n", "import matplotlib.pyplot as plt\n", "import scipy as sp\n", "import numpy as np\n", "\n", "fixed_positions = {0:(10.74,4.07),1:(9.76,6.48),2:(8.39,5.21),3:(10.37,1.98),4:(12.30,5.61),5:(13.31,3.28),6:(13.28,5.00),7:(8.41,7.06),8:(6.72,4.31),9:(5.77,1.38),10:(12.30,2.72),11:(12.75,4.05),12:(11.32,2.41),13:(8.70,2.88),14:(3.33,0.63),15:(1.88,2.01),16:(13.92,4.05),17:(10.77,5.61),18:(0.69,6.40),19:(9.05,1.38),20:(0.34,4.63),21:(11.56,6.22),22:(5.24,0.34),23:(1.88,7.49),24:(5.11,6.80),25:(4.31,8.52),26:(2.14,0.32),27:(3.65,6.64),28:(6.03,5.24),29:(0.77,2.91),30:(7.01,2.43),31:(6.61,7.86),32:(4.60,4.52),33:(4.39,2.91)}\n", "\n", "def plot_karate(G, pr = []): \n", "    if len(pr) :\n", "        nx.draw(G, with_labels=True, pos=fixed_positions, cmap=plt.get_cmap('RdYlBu'), node_color=pr)#node_color=np.array(np.around(kphi[:,i],decimals=1)))\n", "    else : \n", "        nx.draw(G, with_labels=True, pos=fixed_positions)\n", "    \n", "\n", "G = nx.karate_club_graph()\n", "nx.draw(G, with_labels=True)\n", "plt.show()\n", "plot_karate(G)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["It is also easy to manipulate the graph by adding and removing edges or coloring in a different way. "]}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [], "source": ["G.add_edge(2,4)\n", "nx.draw(G, with_labels=True)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["As well as computing the PageRank of nodes, clustering nodes, and perform some algorithmic transformation.\n", "\n", "```python\n", "nx.pagerank(G, alpha=0.85)\n", "```"]}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [], "source": ["pr = nx.pagerank(G, alpha=0.85)\n", "print(pr)\n", "pr_vec = np.fromiter(pr.values(), dtype=float)\n", "plot_karate(G, 1 - pr_vec)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["An important matrix in graphs is the adjacency matrix which gives you structual information about the *connectivity* of the graph. In particular, every row of the adjacency matrix tells you to which other nodes a node is connected. "]}, {"cell_type": "code", "execution_count": 4, "metadata": {}, "outputs": [], "source": ["plt.set_cmap(plt.get_cmap('Blues'))\n", "\n", "A = nx.adjacency_matrix(G)\n", "print(A.todense())\n", "plt.matshow(A.todense())\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Plotted that way the adjacecny matrix doesn't tell you much. However, we can reorder nodes and edges. \n", "**This part of the code used some advanced method you don't need to understand.**\n", "\n", "If ordered proberly we should be able to see the two groups of nodes of the  graph. "]}, {"cell_type": "code", "execution_count": 5, "metadata": {}, "outputs": [], "source": ["from sklearn.cluster import SpectralClustering\n", "\n", "A = nx.adjacency_matrix(G)\n", "\n", "\n", "#### DO NOT TRY TO UNDERSTAND \n", "# The method finds communities in a graph, I just use it to reorder the adjacency matrix. \n", "def reorder_adj_matrix(A, num_clust = 2) : \n", "    sc = SpectralClustering(num_clust, affinity='precomputed', n_init=100)\n", "    sc.fit(A)\n", "    clust = sc.labels_\n", "\n", "    perm = np.argsort(clust)\n", "    perm_mat = np.zeros((len(perm), len(perm)))\n", "\n", "    for idx, i in enumerate(perm):\n", "        perm_mat[idx, i] = 1\n", "\n", "    A_ord = np.dot(np.dot(perm_mat, A.todense()), perm_mat.T)    \n", "    return A_ord, clust\n", "#### DO NOT TRY TO UNDERSTAND\n", "\n", "\n", "A_ord, clust = reorder_adj_matrix(A)\n", "plt.matshow(A_ord)\n", "plt.show()\n", "\n", "print(\"We now see the two communities of nodes\")\n", "plot_karate(G, clust)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Exploratory Task**: You can now play with the small graph, adding and removing edges and see the effect of different values of $\\alpha$ (**note** that in some other notations the $\\alpha$ is called $c$) in the PageRank. Also try Pagerank in different graphs. "]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Exercise 2 - Linear embeddings\n", "\n", "This exercise is about Linear embeddings and how to perform dimensionality reduction on the adjacency matrix or a matrix derived from the adjacency matrix. \n", "\n", "Linear embeddings typically perform some linear dimensionality reduction on, variations of the Adjacency matrix $A\\in \\{0,1\\}^{n \\times n}$ for an **undirected graph** $G = (V,E)$. \n", "\n", "A linear embedding finds a matrix $Z \\in \\mathbb{R}^{n\\times d}$, such that the i'th row of $Z$ is the $d$-dimensional representation of node (or vertex) d in the graph.\n", "\n", "The methods for learning such an embedding matrix $Z$ try to minimize a \"reconstruction error\" of the embedding with respect to the adjacecy matrix\n", "which is formalized in the following loss function $\\mathcal{L}$.\n", "$$\n", "\\mathcal{L}(A,Z) = \\sum_{(i,j) \\in E}(Z_{i}^\\top Z_{j} - A_{i,j})^2 = \\|ZZ^\\top - A\\|_F^2\n", "$$\n", "\n", "1. Show that if instead of learning one matrix Z, you are allowed to learn two matrices $B \\in \\mathbb{R}^{n\\times d}$ and $C \\in \\mathbb{R}^{n\\times d}$, the best solution to the above minimization can be obtained by SVD on $A$.\n", "\n", "\n", "2. What is the gradient matrix of the above function for a single row $Z_{i\\cdot}$ of the matrix $Z$?\n", "\n", "\n", "3. Implement the linear embedding above using gradient descent and $\\hat{A}$ the adjacency matrix\n", "\n", "\n", "4. \\[Optional\\] Implement the same procedure using pytorch. "]}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": false}, "outputs": [], "source": ["np.random.seed(1286214515)\n", "\n", "def lin_grads(A, Z): \n", "    \"\"\" Compute gradient of loss relative relative to all entries in Z\n", "    \n", "        Args: \n", "            A: np.array shape n x n\n", "            Z: np.array shape n x d\n", "        \n", "        Returns:\n", "            res: np.array shape n x d\n", "    \"\"\"\n", "    res = np.zeros(Z.shape)\n", "    ### YOUR CODE HERE    \n", "    ### END CODE\n", "    return res\n", "\n", "def lin_loss(A, Z): \n", "    \"\"\" Compute gradient (matrix) of derivatives of loss relative relative to all entries in Z\n", "    \n", "        Args: \n", "            A: np.array shape n x n\n", "            Z: np.array shape n x d\n", "            \n", "        Returns:\n", "            res: float scaler, \n", "    \"\"\"\n", "    res = 0\n", "    ### YOUR CODE HERE    \n", "    ### END CODE\n", "    return res\n", "\n", "    \n", "def learn(A, dim, grad = lin_grads, loss = lin_loss, step_size=0.001, steps=1000):\n", "    \"\"\" Gradient Descent learning Algorithm \n", "    \n", "        Args:\n", "            A: np.array shape n x n\n", "            dim: int dimension of embedding\n", "            grad: function - gradient computation function\n", "            loss: function - loss computation function\n", "            step_size: float - learning rate for gradient descent\n", "            steps: int - number of iterations of gradient descent\n", "\n", "        Returns:\n", "            Z: np.array n x d - the learned parameters                \n", "    \"\"\"\n", "    Z = np.random.rand(A.shape[0], dim)\n", "    i = 0\n", "    ### YOUR CODE HERE    \n", "    ### END CODE\n", "    return Z\n", "\n", "# Project the graph in 2D and plot it as points in the space\n", "Z = learn(A, 2)\n", "\n", "# As colors to see what happens we can use the communities \n", "# that have been found with the magic method in the beginning.\n", "# A good embedding method should be able to separate the two colors! \n", "colors = ['red' if c == 0 else 'blue' for c in clust]\n", "plt.scatter(np.array(Z[:,0]), np.array(Z[:,1]), c=colors)\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Exercise 3 - Random walks and PageRank\n", "\n", "Many embedding methods rely on random walks to capture the structure of the neighbors of a node. Random walks are tightly connected to the Pagerank as a limit of convergence in performing random walks on a graph. \n", "\n", "In this theoretical exercise we will reason about PageRank and its properties. Remember that the Personalized Pagerank in a undirected graph can be experssed by the equation \n", "$$\n", "\\mathbf{p} = \\alpha W\\mathbf{p} - (1-\\alpha)\\mathbf{r} \n", "$$\n", "where $W = A^\\top D^{-1}$ is the transition matrix, $\\mathbf{r}$ is called the restart vector and represents the starting nodes of the random walk where the walker will jump back with probability $(1-\\alpha)$. \n", "\n", "To compute the Pagerank vector one could \n", "\n", "Let us assume $\\alpha = 0.85$ and $\\mathbf{r} = \\frac{1}{n}\\mathbf{1}_n$ is uniformly distributed where $\\mathbf{1}_n$ is the $n$-dimensional vectors with all 1s. \n", "\n", "**Optional**: Prove that $A^\\top D^{-1}$ corresponds to the transition matrix.\n", "\n", "\n", "1. What is the PageRank of a complete graph of n nodes? \n", "\n", "\n", "2. What is the PageRank of a complete bipartite graph with 2 nodes on one side and 4 on the other side and $\\alpha=1$? \n", "\n", " \n", "3. A path graph is a graph where each node is connected to the consecutive. What is the PageRank with 3 nodes when $\\alpha=1$? \n", "**Hint**: Remember that the elements in the PageRank vector sum to 1 (it's a distribution over nodes). \n", "\n", "\n", "4. Provide an implementation of the Power Iteration method  in the cell below\n", "\n", "5. Check your code, why does the method fails for the path graph and $\\alpha = 1$?\n", "\n"]}, {"cell_type": "code", "execution_count": 7, "metadata": {}, "outputs": [], "source": ["from networkx.algorithms import bipartite\n", "\n", "# Tolerance is the sum of the absolute differences among the pagerank vectors in consequent iterations\n", "# The iteration stops is the the differences are less than the tolerance\n", "def my_pagerank(G, alpha = 0.85, r = None, max_iter = 100, tol=1e-06) : \n", "    \"\"\" Compute pagerank with power method\n", "    \n", "        Args:\n", "            G: networkx graph\n", "            alpha: float, Page rank parameters\n", "            r: np.array shape n,\n", "            max_iter: int - max iteration of power method\n", "            tol: float - stopping criterion tolerance. \n", "        Returns\n", "            p: np.array n, - the page rank of each node\n", "    \"\"\"\n", "    A = nx.adjacency_matrix(G)\n", "    n = G.number_of_nodes()\n", "    p = np.full(n, 1/n)\n", "\n", "    if r is None : \n", "        r = np.full(n, 1/n)\n", "\n", "    ### YOUR CODE HERE\n", "    ### END CODE\n", "    return p\n", "\n", "Gb = nx.complete_bipartite_graph(4, 5, create_using=None)\n", "print('Pagerank bipartite')\n", "print(nx.pagerank_numpy(Gb, alpha=1))\n", "print(my_pagerank(Gb))\n", "\n", "\n", "Gc = nx.complete_graph(5)\n", "print('Pagerank complete')\n", "print(nx.pagerank(Gc, alpha=0.85))\n", "print(my_pagerank(Gc))\n", "\n", "Gl = nx.path_graph(3)\n", "print('Pagerank path graph')\n", "print(nx.pagerank_numpy(Gl, alpha=1))\n", "print(my_pagerank(Gl, alpha=1))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Exercise 4 - VERSE\n", "\n", "In this exercise, we will try to implement and run one network embedding introduced in the lecture, called [VERSE](https://arxiv.org/pdf/1803.04742.pdf).\n", "\n", "VERSE takes as input a graph $G=(V,E)$, a row-normalized similarity matrix $S$ and computes an embedding matrix $Z \\in \\mathbb{R}^{n\\times d}$ by  minimizing the following cross-entropy loss between the rows in $S$ and the rows in $ZZ^\\top$.\n", "\n", "The VERSE loss function for each node $i$ is \n", "$$\n", "\\mathcal{L}(S_i,Z) = -S_i \\log(\\text{softmax}(Z_iZ^\\top)) \n", "$$\n", "\n", "which gives a total global loss over all nodes as \n", "$$\n", "\\mathcal{L}(S,Z) = \\sum_{i \\in V}\\mathcal{L}(S_i,Z)\n", "$$\n", "here $\\text{softmax}(\\cdot)$ is the [softmax](https://en.wikipedia.org/wiki/Softmax_function) function. \n", "\n", "The similarity matrix in case of VERSE is typically the Personalized PageRank (PPR) matrix, which is obtained row-by-row running the PageRank algorithm with $\\alpha = 0.85$.\n", "\n", "The PPR matrix is an $n \\times n$ matrix where the i'th row is the Page rank vector given by the restart vector that is the indicator vector for node i i.e. a vector with all zeros except a one in position i.\n", "\n", "\n", "We will first implement the full VERSE method and understand how it works in practice \n", "1. Compute the Personalized PageRank matrix starting the random walk computation from all the nodes individually using the formula above or iterating with my_pagerank\n", "2. Implement the VERSE Loss function in verse_loss below\n", "\n", "3. Compute the gradient of the loss function and implement it in verse_grads. If you cannot come up with the gradient you can instead use pytorch in the following cell, where you must implement the loss in pytorch \n", "**Hint: do it row-wise (i.e., node-by-node) and remember the gradient of cross-entropy with softmax activation**\n", "\n", "\n", "3. Try to change the parameters and use another similarity function. What other similarity function among nodes can you use? "]}, {"cell_type": "code", "execution_count": 66, "metadata": {"scrolled": false}, "outputs": [], "source": ["from scipy.special import softmax\n", "\n", "def pagerank_matrix(G, alpha = 0.85):\n", "    \"\"\" Compute the personalized Page Rank Matrix of G\n", "        \n", "        Args:\n", "            G: networkx graph\n", "            alpha: float\n", "    \n", "        returns\n", "        np.array number_of_nodes(G) x number_of_nodes(G)\n", "    \"\"\"\n", "    n = G.number_of_nodes()\n", "    A = nx.adjacency_matrix(G).todense()    \n", "    ### YOUR CODE HERE\n", "    ### END CODE\n", "\n", "P = pagerank_matrix(G)\n", "n = G.number_of_nodes()\n", "\n", "# Sanity checks on the matrix\n", "per = np.zeros(n)\n", "per[1] = 1\n", "print(my_pagerank(G,r=per))\n", "print (P[1, :])\n", "\n", "print(P.sum(axis=1)) # Every row should sum to 1.\n", "\n", "\n", "def verse_loss(S, Z): \n", "    \"\"\" Compute the loss of VERSE \n", "    \n", "        Args:\n", "            S: np.array shape n x n\n", "            Z: np.array shape n x d\n", "            \n", "        Returns:\n", "            loss: float    \n", "    \"\"\"\n", "    loss = 0\n", "    ### YOUR CODE HERE\n", "    ### END CODE\n", "    return loss\n", "\n", "verse_loss(.5 *np.ones((2,2)),np.array([[1,2],[3,4]])) \n", "\n", "def verse_grads(S, Z): \n", "    \"\"\" Compute the loss of VERSE \n", "    \n", "        Args:\n", "            S: np.array shape n x n\n", "            Z: np.array shape n x d\n", "            \n", "        Returns:\n", "            np.array shape n x d    \n", "    \"\"\"\n", "\n", "    grads = np.zeros(Z.shape)\n", "    ### YOUR CODE HERE \n", "    ### END CODE\n", "    return grads\n", "\n", "Z = learn(P, 2, grad=verse_grads, loss=verse_loss, step_size = 0.005, steps=4000)\n", "fig, ax = plt.subplots(1, 1, figsize=(12,10))\n", "ax.scatter(Z[:,0], Z[:,1], c=colors)\n", "ax.set_title('Verse Embedding: Colors are Cluster colors from earlier', fontsize=20)\n", "plt.show()\n"]}, {"cell_type": "code", "execution_count": 68, "metadata": {}, "outputs": [], "source": ["import torch\n", "import torch.nn\n", "from torch import optim\n", "\n", "\n", "def t_verse_loss(S, Z):\n", "    \"\"\" Verse Loss function in pytorch \n", "        \n", "        torch.nn.functional.log_softmax may come in handy\n", "        \n", "        Args:\n", "            S: torch.tensor shape (n x n)\n", "            Z: torch.tensor shape (n x d)\n", "    \n", "        Returns:\n", "            loss: float scalar            \n", "    \"\"\"\n", "    loss = 0\n", "    n = S.shape[0]\n", "    ### YOUR CODE HERE\n", "    ### END CODE\n", "    return loss\n", "\n", "def t_fit(target_matrix, d, epochs, lr):\n", "    \"\"\" Pytorch gradient descent \n", "    \n", "        Args:\n", "            target_matrix: torch.tensor shape (n x n), the similary matrix  to approximate\n", "            d: int, positive embedding dimension\n", "            eposh: int, number of iterations to run\n", "            lr: float - learning rate of Gradient Descent\n", "    \n", "        Returns\n", "            W: torch.tensor shape (n x d) - the learned embedding\n", "    \"\"\"\n", "    n = target_matrix.shape[0]\n", "    W = torch.rand(n, d, requires_grad=True)\n", "    optimizer = optim.SGD(params={W}, lr=lr)\n", "    for i in range(epochs):\n", "        optimizer.zero_grad()\n", "        loss = t_verse_loss(target_matrix, W)\n", "        loss.backward()\n", "        if i % 20 == 0:\n", "            print(f'epoch {i}: verse loss: {loss.item()}\\r', end='')\n", "        optimizer.step()\n", "    print('\\nfinal loss', loss.item())\n", "    return W.detach()\n", "torch_P = torch.from_numpy(P).float()\n", "Z = t_fit(target_matrix=torch_P, d=2, epochs=4000, lr=0.005).numpy()\n", "fig, ax = plt.subplots(1, 1, figsize=(12,10))\n", "ax.scatter(Z[:,0], Z[:,1], c=colors)\n", "ax.set_title('Verse Embedding: Colors are Cluster colors from earlier', fontsize=20)\n", "plt.show()\n", "    "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.4"}}, "nbformat": 4, "nbformat_minor": 2}