{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Exercises Week 10\n", "Week 10 exercises is about training word embedding using the Noise Contrastive Estimation (NCE) Skip Gram-model and CBOW model.\n", "\n", "In my_util.py we have included standard code for loading data, training a neural network model with pytorch, (as you have seen in earlier exercises) and some nearest neighbor similarity search.\n", "\n", "Your task is simply to implement the neural network architectures required for training the embedding model.\n", "\n", "Before we get to that let us start with the basic Skip-Gram Model and how we train it.\n", "In the classix Skip Gram model we learn embeddings by teaching a simple neural net: to learn how to predict the neighbouring words in a sentence.\n", "\n", "\n", "The code for the Skip Gram architecture is shown  in the cell below.\n", "Notice, the model has two matrices:\n", "- the embedding matrix named embedding of size $v \\times d$, where $v$ is the number of different words and d is the dimension of the embedding. The i'th row of the matrix is the embedding vector for the i'th word in the list of all considered words.\n", "- the softmax output matrix named linear of size $d \\times v$ that maps an embedding vector of size d to $v$ numbers, one entry for each word and these define the model estimates of which words are likely to be near the input word. To make the values into \"probabilities\" the $v$ numbers are passed through the softmax function.\n", "\n", "The forward pass simply looks up the current embedding vector for each input point and then multiplies that vector with the weights defined in linear (just like a standard linar model) and returns the result. The loss used is Negative Log Likelihood on softmax transformation of the output, known as Cross Entropy.\n", "\n", "Note that the training data to train such a skip gram model can be extracted from any test and that both the input and the text is an interger in $\\{1,\\dots, v\\}$ both encoding the index of a word.\n"]}, {"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [], "source": ["%matplotlib inline\n", "import torch\n", "import torch.utils.data as torch_data\n", "import matplotlib.pyplot as plt\n", "import torch.nn as nn\n", "import torch.optim as optim\n", "import torch.nn.functional as F\n", "import numpy as np\n", "import json\n", "\n", "class SkipGram(nn.Module):\n", "\n", "    def __init__(self, vocab_size, embedding_dim):\n", "        \"\"\" Construct and save the parameters needed for the model \n", "        \n", "            The embedding parameters must be stored in self.embedding\n", "        \"\"\"\n", "\n", "        super(SkipGram, self).__init__()\n", "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n", "        self.linear = nn.Linear(embedding_dim, vocab_size)\n", "\n", "    def forward(self, inputs):\n", "        \"\"\" Evaluation of the neural net. The forward pass \n", "        \n", "            Args:\n", "                inputs: torch.tensor n x 1 (each data point is a word index)\n", "                \n", "            Returns:\n", "                out: torch.tensor shape n x v\n", "        \"\"\"\n", "        embeds = self.embedding(inputs).squeeze() # batch_size, e_dim \n", "        #print(embeds.shape)\n", "        out = self.linear(embeds)\n", "        return out\n", "    \n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Exercise 1: Noise Contrastive Estimation for Skip-Gram\n", "To get around the problem of updating the embedding for every word in each iteration of gradient descent which makt take very long, we instead train a skip gram embedding model with Noise Contrastive Estimation (NCE),  which changes the problem into a binary classification problem, where the neural net is tasked to learn to determine whether two words comes from the same sentence context or not.\n", "\n", "In NCE each input data point is two words and we predict whether they are from the same sentence context, where in the classic skip gram we try to predict word two from word 1. These two words are represented as indexes i.e. word 7 and word 15 which is simply represented as the vector $[7, 15]$ and the label is an integer in {0, 1} indicating whether the two words are taken from the same real sentence context or it is a noise sample created by us.\n", "\n", "The neural net architecture works  as follows on a fixed input point (note that input to neural net is batches)\n", "- Step 1: Compute the embedding vectors for the two inputs \n", "- Step 2: Computer the inner product of the two embeddings\n", "- Step 3: Return this value \n", "\n", "Implement the architecture in the cell below in SkipGramNCE. \n", "Remeber to name the embedding layer (embedding i.e. self.embedding)"]}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [], "source": ["class SkipGramNCE(nn.Module):\n", "\n", "    def __init__(self, vocab_size, embedding_dim):\n", "        \"\"\" Construct and sace the parameters needed for the model \n", "        \n", "            The embedding parameters must be stored in self.embedding\n", "        \"\"\"\n", "    \n", "        super(SkipGramNCE, self).__init__()\n", "        ### YOUR CODE HERE\n", "        ### END CODE\n", "\n", "    def forward(self, inputs):\n", "        \"\"\" Evaluation of the neural net. The forward pass \n", "        \n", "            Args:\n", "                inputs: torch.tensor n x 2 (each data point is a pair of word indexes)\n", "                \n", "            Returns:\n", "                out: torch.tensor shape n,\n", "        \"\"\"\n", "        out = None\n", "        ### YOUR CODE HERE\n", "        ### END CODE\n", "        return out\n", "\n", "# trivial test to see if it computes something\n", "smodel = SkipGramNCE(5, 10)\n", "test_data = torch.tensor([[2, 3], [3, 1]])\n", "res = smodel.forward(test_data)\n", "print(res)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Exercise 1 continued: Testing on Large Data\n", "To test your skip gram NCE implementation run the cell below that also shows the nearest words for a few examples that you can experiment with if you like. You can also change the embedding dimension if you like.\n", "\n", "In the following three cells we test your implementation\n", "- cell 1: load the data (slow so separate cell) - you can load a small data set for testing if need be (-1 means all data)\n", "- cell 2: training (slow so separate cell) particularly if use all the data and many and set embedding to high as 150 or 200\n", "- cell 3: printing some nearest neighbours in embedding space, results should improve with data size and training time"]}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [], "source": ["import my_util \n", "data_path = './skip_grams.json'\n", "### YOUR CODE HERE - set data path correct here\n", "### END CODE\n", "dataset, dataloader, words_to_idx = my_util.load_skipgram_data(data_path, data_size=-1)\n"]}, {"cell_type": "code", "execution_count": 4, "metadata": {"scrolled": false}, "outputs": [], "source": ["print('Index of horse:', words_to_idx['horse'])\n", "tmp = dataset[0]\n", "print(tmp)\n", "print(words_to_idx.inverse[1516])\n", "print('first data point: {0} -> {1}: label {2}'.format(words_to_idx.inverse[tmp[0][0].item()], \n", "                                                      words_to_idx.inverse[tmp[0][1].item()], tmp[1]))\n", "# Settings\n", "embedding_dim = 100\n", "vocab_size = len(words_to_idx)\n", "skip_net = SkipGramNCE(vocab_size, embedding_dim)\n", "\n", "hist = my_util.fit_model(skip_net, dataloader, epochs=3)"]}, {"cell_type": "code", "execution_count": 5, "metadata": {}, "outputs": [], "source": ["print(vocab_size, embedding_dim)\n", "skip_embedding = skip_net.embedding.weight.data\n", "knn = my_util.KNN(skip_embedding, words_to_idx)\n", "test_words = [\"three\", \"cat\", \"city\", \"player\", \"king\", \"queen\"]\n", "knn.print_nearest(test_words)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Exercise 2: Noise Contrastive Estimation for CBOW\n", "This exercise is very similar to exercise 1, instead now we use NCE for the CBOW model instead of the Skip-Gram model. The difference to skip gram is that now instead of using one word to predict the surrounding words, we make an algorithm that tries to predict the middle word given the surrounding words. Again we use NCE and get a binary classifation problem. \n", "\n", "The input is now a k+1 dimensional vector  where $k$ is the size of the context used (number of words) around the word we need to predict. The first $k$ values are the indexes of the $k$ words in the context and the last entry is is the index of the target word. The label is again a value in {0, 1} indicating whether the data comes from a real sentence or a fake generated one. \n", "\n", "The neural net architecure is as follows on a given input $x = [x_0,\\dots, x_k]$ (note that the input comes in batches - here we describe one point)\n", "- Step 1: Compute the embedding vectors for all the inputs indices\n", "- Step 2: Compute the mean of the embedding vectors for the first k words (all except the last)\n", "- Step 3: Computer the inner product between this mean embedding vector and the embedding for the last word\n", "- Step 4: Return this value\n", "\n", "Implement the architecture in the cell below in CBOW_NCE.\n", "Remeber to name the embedding layer (embedding i.e. self.embedding)"]}, {"cell_type": "code", "execution_count": 6, "metadata": {}, "outputs": [], "source": ["class CBOW_NCE(nn.Module):\n", "\n", "    def __init__(self, vocab_size, embedding_dim):\n", "        \"\"\" Construct and sace the parameters needed for the model \n", "        \n", "            The embedding parameters must be stored in self.embedding\n", "        \"\"\"\n", "    \n", "        super(CBOW_NCE, self).__init__()\n", "        ### YOUR CODE HERE\n", "        ### END CODE\n", "\n", "    def forward(self, inputs):\n", "        \"\"\" Evaluation of the neural net. The forward pass \n", "        \n", "            Args:\n", "                inputs: torch.tensor n x 2 (each data point is a pair of word indexes)\n", "                \n", "            Returns:\n", "                out: torch.tensor shape n,\n", "        \"\"\"\n", "        out = None\n", "        ### YOUR CODE HERE\n", "        ### END CODE\n", "        return out\n", "\n", "# trivial test to see if it computes something\n", "smodel = CBOW_NCE(20, 10)\n", "test_data = torch.tensor([[1, 2, 3, 4, 5], [3, 3, 3, 3  , 10]])\n", "res = smodel.forward(test_data)\n", "print(res)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Exercise 2 continued: Testing on Large Data\n", "To test your CBOW NCE implementation run the cell below that also shows the the nearest words for a few examples that you can experiment with if you like. You can also change the embedding dimension if you like.\n", "\n", "In the following three cells we test your implementation\n", "- cell 1: load the data (slow so separate cell) - you can load a small data set for testing if need be (-1 means all data)\n", "- cell 2: training (slow so separate cell) particularly if use all the data and many and set embedding to high as 150 or 200\n", "- cell 3: printing some nearest neighbours in embedding space, results should improve with data size and training time"]}, {"cell_type": "code", "execution_count": 7, "metadata": {}, "outputs": [], "source": ["import my_util \n", "data_path_cbow = './cbow.json'\n", "### YOUR CODE HERE - set data path correct here\n", "### END CODE\n", "dataset_cbow, dataloader_cbow, words_to_idx_cbow = my_util.load_cbow_data(data_path_cbow, data_size=-1)\n", "\n"]}, {"cell_type": "code", "execution_count": 8, "metadata": {}, "outputs": [], "source": ["print('Index of horse:', words_to_idx_cbow['horse'])\n", "tmp = dataset_cbow[0]\n", "print(tmp)\n", "print(words_to_idx_cbow.inverse[1516])\n", "print('first data point: {0} -> {1}: label {2}'.format([words_to_idx_cbow.inverse[tmp[0][i].item()] for i in range(4)],\n", "                                                      words_to_idx_cbow.inverse[tmp[0][4].item()], tmp[1]))\n", "# Settings\n", "embedding_dim_cbow = 100\n", "vocab_size_cbow = len(words_to_idx_cbow)\n", "cbow_net = CBOW_NCE(vocab_size_cbow, embedding_dim_cbow)\n", "\n", "hist = my_util.fit_model(cbow_net, dataloader_cbow, epochs=20)"]}, {"cell_type": "code", "execution_count": 9, "metadata": {}, "outputs": [], "source": ["print(vocab_size_cbow, embedding_dim_cbow)\n", "cbow_embedding = cbow_net.embedding.weight.data\n", "knn = my_util.KNN(cbow_embedding, words_to_idx_cbow)\n", "test_words = [\"three\", \"cat\", \"city\", \"player\", \"king\", \"queen\"]\n", "knn.print_nearest(test_words)\n", "\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.4"}}, "nbformat": 4, "nbformat_minor": 2}