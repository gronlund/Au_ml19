{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Week 4 (Theoretical) Exercises\n", "Remember to take a look at all the exercises.\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Ex 1: Gradient Descent \n", "Let $f_a(x_1, x_2) = \\frac{1}{2}(x_1^2 + a\\cdot x_2^2)$\n", "\n", "Where $a$ is parameter that we will change.\n", "\n", "**Your task is to write a gradient descent algorithm that finds a minimizer of $f$, where we have decided that the starting point for the gradient descent is (256, 1). It should be possible for you to figure out what the local (global) minimum is, as well as the gradient.**\n", "- Test your algorithm by running the cell.\n", "- run your gradient descent algorithm for at least 40 steps to see if it converges. \n", "You must save the sequence of elements (2d points) considered in your gradient descent algorithm for visualization. \n", "We have added code to visualize this sequence.\n", "\n", "- Try a=1, 4, 16, 64, 128, 256 and adjust the step size to see if you can make it converge.\n", "    **hint - after trying different values for the stepsize also try approximately 1/a (for a > 1)**\n", "- What do you see? \n", "\n", "\n", "\n"]}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [], "source": ["%matplotlib inline\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "\n", "def f(a, x):\n", "    return 0.5 * (x[0]**2 + a * x[1]**2)\n", "\n", "def visualize(a, path, ax=None):\n", "    \"\"\"\n", "    Make contour plot of f_a and plot the path on top of it\n", "    \"\"\"\n", "    y_range = 10\n", "    x = np.arange(-257, 257, 0.1)\n", "    y = np.arange(-y_range, y_range, 0.1)\n", "    xx, yy = np.meshgrid(x, y)\n", "    z = 0.5 * (xx**2 + a * yy**2)\n", "    if ax is None:\n", "        fig, ax = plt.subplots(figsize=(16, 13))\n", "    h = ax.contourf(xx, yy, z, cmap=plt.get_cmap('jet'))\n", "    ax.plot([x[0] for x in path], [x[1] for x in path], 'w.--', markersize=4)\n", "    ax.plot([0], [0], 'rs', markersize=8) # optimal solution\n", "    ax.set_xlim([-257, 257])\n", "    ax.set_ylim([-y_range, y_range])\n", "\n", "def gd(a, step_size=0.1, steps=40):\n", "    \"\"\" Run Gradient descent\n", "        params:\n", "        a - the parameter that define the function f\n", "        step_size - constant stepsize to use for gradient descent\n", "        steps - number of steps to run\n", "        \n", "        Returns: out, list with the sequence of points considered during the descent.         \n", "    \"\"\"\n", "    out = []\n", "    x = np.array([256.0, 1.0]) # starting point\n", "\n", "    ### YOUR CODE HERE    \n", "    ### END CODE\n", "    return out\n", "\n", "fig, axes = plt.subplots(2, 3, figsize=(20, 16))\n", "ateam = [[1, 4, 16], [64, 128, 256]]\n", "for i in range(2):\n", "    for j in range(3):\n", "        ax = axes[i][j]\n", "        a = ateam[i][j]\n", "        path = gd(a, step_size=0.1, steps=40) # use good step size here instead of standard value\n", "        visualize(a, path, ax)\n", "        ax.set_title('Gradient Descent a={0}'.format(a), fontsize=16)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Exercise 2: In Sample Error\n", "Assume we are given a fixed hypothesis h, and we are considering 0-1 loss ($1_{h(x)\\neq y}$).\n", "Now we receive a sample data set $D = {(x_1,y_1),\\dots,(x_n,y_n)}$ where each data point is generated by sampling $x_i$  independently at random from unknown distribution $P(X)$ and then fed to the unknown $f$ to get $(x_i, y_i)$, where $y_i = f(x_i)$.\n", "\n", "\n", "Show that the expected value (over the data set) of $E_{in}(h) = \\frac{1}{n} \\sum_{i=1}^n 1_{h(x_i)\\neq y_i}$ ? (number of mispredictions/number of points) is $E_{out}(h)$.\n", "\n", "Formally show that\n", "$$\n", "\\mathbb{E}_D [\\textrm{E}_\\textrm{in}(h)] = \\textrm{E}_\\textrm{out}(h)\n", "$$\n", "\n", "\n", " \n", "# Exercise 3: Out of sample error \n", "Given that the target function is actually the the noisy linear model, \n", "$$ P(y\\mid x,w) = w^\\intercal x + \\varepsilon $$ \n", "where $\\varepsilon$ is a random variable with mean zero, $\\mathbb{E}[\\varepsilon] = 0$, and standard deviation $\\sigma$.\n", "\n", "\n", "Given that we use the least squares error function, $e(x,y) = (x-y)^2$,\n", "what is the best **out of sample error** possible?\n", "\n", "Hint: What is the optimal classifier? What is the out of sample error of\n", "that one?\n", "\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Exercise 4: Softmax Gradient\n", "As described in the softmax note, we define the softmax function as follows:\n", "$$\n", "\\textrm{softmax}:\\mathbb{R}^K \\rightarrow \\mathbb{R}^K, \\quad\n", "\\textrm{softmax}(x)_j =\n", "\\frac{e^{x_j}}\n", "{\\sum_{i=1}^K e^{x_i}}\\quad\n", "\\textrm{ for }\\quad j = 1, \\dots, K.\n", "$$\n", "where  $\\textrm{softmax}(x)_j$ denote the $j$'th output of the function\n", "\n", "\n", "Show that the matrix of derivatives of the softmax function is as follows.\n", "$$\n", "\\left[\\frac{\\partial \\textrm{softmax}}{\\partial x}\\right]_{i,j} =\n", "\\frac\n", "{\\partial \\;\\textrm{softmax}(x)_i}\n", "{\\partial x_j} =\n", "(\\delta_{i,j} - \\textrm{softmax}(x)_j)\n", "\\textrm{softmax}(x)_i\\quad\\quad\\text{where}\\quad\\quad\n", "\\delta_{ij}=\\begin{cases}1 &\\text{if }i=j\\\\\n", "0 & \\text{else}\n", "\\end{cases}\n", "$$\n", "\n", "\n"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.4"}}, "nbformat": 4, "nbformat_minor": 2}