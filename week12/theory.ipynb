{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 12 Exercises\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 1:  Bias Variance \n",
    "\n",
    "-   Does Bias and Variance terms (two numbers) in the Bias Variance\n",
    "    decomposition depend on the learning algorithm.\n",
    "\n",
    "-   What is Variance (in Bias Variance tradoff) if we have a hypothesis\n",
    "    set of size $1$ namely the constant model $h(x) = 2$. The learning\n",
    "    algorithm always picks this hypothesis no matter the data.\n",
    "\n",
    "-   What is the Variance (in the Bias Variance tradeoff) if the simple\n",
    "    hypothesis from the previous question is replaced by a very very\n",
    "    sophisticated hypothesis.\n",
    "\n",
    "-   Assume the target function is a second degree polynomial, and the\n",
    "    input to your algorithm is always eleven distinct (noiseless) points. Your\n",
    "    hypothesis set is the set of all degreee 10 polynomial and the\n",
    "    learning algorithm returns the hypothesis with the best fit\n",
    "    (miniming least squared error) given the data. What is Bias and what\n",
    "    is Variance?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 2: Regularization with Weight decay\n",
    "If we use weight decay regularization ($\\lambda||w||^2)$  for some real number $\\lambda$ in Linear Regression what \n",
    "happens to the optimal weight vector if we let $\\lambda \\rightarrow \\infty$? (cost is $\\frac{1}{n} \\|Xw - y\\|^2 + \\lambda \\|w\\|^2$)\n",
    "\n",
    "**Hard:** Can you say something about the changes in behaviour of the optimal solution $w$ as $\\lambda$ decreases from $0$ towards $-\\infty$. When does the optimal cost change from something finite to $-\\infty$?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 3: Bias Variance - Hard Exercise\n",
    "Book Problem 2.24 part (a)\n",
    "\n",
    "Short Version:\n",
    "   \n",
    "  - The target function is $f(x) = x^2$ and the cost is Least Squares.\n",
    "\n",
    "  - Sample two points $x_1, x_2$ from $[-1, 1]$ uniformly at random to get the data set $D = \\{(x_1, x_1^2), (x_2, x_2^2)\\}$\n",
    "\n",
    "  - Use hypothesis space $\\{h(x) = ax +b\\mid a,b\\in\\Bbb R\\}$ i.e. lines. There are two parameters $a$ and $b$.\n",
    "\n",
    "  - Given a data set $D = \\{(x_1, x_1^2), (x_2, x_2^2)\\}$ the algorithm returns the line that fits these points.\n",
    "\n",
    "  - Your task is to write down an analytical expression for $\\bar{g} = \\mathbb{E}_D [h_D]$ where $h_D$ is the hypothesis learned on D.\n",
    "\n",
    "**Step 1.** What is the in sample error of $h_D$ and why?\n",
    "\n",
    "**Step 2.** Given $D$ what are $a, b$ (defined by the line between $(x_1, x_1^2)$ and  $(x_2, x_2^2)$)? Hint: $x_2^2- x_1^2 = (x_2-x_1)(x_2 + x_1)$.\n",
    "\n",
    "**Step 3.** What is the expected value of the slope $a$ over $x_1$ and $x_2$?\n",
    "\n",
    "**Step 4.** What is the expected value of the intercerpt $b$ over $x_1$ and $x_2$? \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**More hints**\n",
    "For the uniform distribution over $[-1,1]$ the mean is $0$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 4: Bias Variance Experiment \n",
    "In this exercise you must redo the experiment shown at the lectures.\n",
    "This exercise takes up quite a lot of space so we have moved it to a separate notebook. Go to [BiasVariance Notebook](BiasVariance.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 5: Grid Search For Regularization and Validation - Sklearn\n",
    "In this exercise you must we will optimize a [Decision Tree Classifier](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) using regularization and validation.\n",
    "You must use the in grid search module [GridSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV) from sklearn.\n",
    "\n",
    "In the cell below we have shown an example of how to use the gridsearch module test two different values for max_depth for a a decision tree for wine classification\n",
    "\n",
    "Your job is to good hyperparameters for decision trees for the breast cancer detection.\n",
    "\n",
    "### Task 1:\n",
    "For the breast cancer data set find the best (or very good) combination of max_depth and  min_samples_split  (cell two below)\n",
    "\n",
    "The **max_depth** parameter controls the max depth of a tree and the deeper the tree the more complex the model.\n",
    "\n",
    "The **min_samples_split** controls how many elements the algorithm that constructs the tree is allowed to try and split.\n",
    "So if a subtree contains less than min_leaf_size elements it many not be split into a larger subtree by the algorithm.\n",
    "\n",
    "\n",
    "### Task 2:\n",
    "- How long time does it take to use grid search validation for $k$ hyperparamers where we test each parameter for $d$ values, and the training algorithm uses f(n) time to train on n data points where we split the data into 5 parts.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine, load_breast_cancer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "from sklearn.datasets import fetch_covtype\n",
    "\n",
    "def show_result(clf):\n",
    "    df = pd.DataFrame(clf.cv_results_)\n",
    "    df = df.sort_values('mean_test_score', ascending=False)\n",
    "    display(df)\n",
    "    print('best parameter found', clf.best_params_)\n",
    "    \n",
    "w_data = load_wine()\n",
    "wine_data = w_data.data\n",
    "wine_labels = w_data.target\n",
    "\n",
    "# grid search validation\n",
    "reg_parameters = {'max_depth': [1, 30]}  # dict with all parameters we need to test\n",
    "clf = GridSearchCV(DecisionTreeClassifier(), reg_parameters, cv=3, return_train_score=True)\n",
    "clf.fit(wine_data, wine_labels)\n",
    "# code for showing the result\n",
    "bt = show_result(clf)\n",
    "                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_data = load_breast_cancer()\n",
    "c_data = cancer_data.data\n",
    "c_labels = cancer_data.target\n",
    "\n",
    "\n",
    "def decisiontree_model_selection(train_data, labels):\n",
    "    clf = None\n",
    "    ### YOUR CODE HERE\n",
    "    ### END CODE\n",
    "    return clf\n",
    "###\n",
    "clf = decisiontree_model_selection(c_data, c_labels)\n",
    "bt = show_result(clf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
