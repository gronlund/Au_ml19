{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Exercises Week 5"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Exercise 1: Neural Network Design\n", "In this exercise your task is to neural networks by hand that compute simple functions.\n", "For the nonlinear transform you can mix them any way you like but you can only use, identity, sign, relu and sigmoid transforms in the neurons.\n", "You can make the networks as wide and deep as you would like but small networks are sufficient.\n", "* Make a network that computes $c \\cdot x$ for any constant c\n", "* Make a network that computes xor of inputs $x_1$ and $x_2$, where $x_1, x_2 \\in \\{0, 1\\}$\n", "* Make a network that computes max($x_1$,$x_2$)\n", "* Make a network that computes $x^2$ - for any integer x in the set {2,3,4,5} \n", "\n", "\n", "- **Hint 1: It may be easier to find an easy mathematical expression that solves the problem and then to make a network that implements that**\n", "- **Hint 2: The only nonlinear transforms the lecturer use are relu and identity**.\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Ex 2: Neural Net Forward Pass - Vectorized\n", "Implement the score (least squares error), and predict function for a a neural net class for regression.\n", "The neural net has one hidden layer with  $\\textrm{relu}(x) = \\max(0, x)$ nonlinearity and one output neuron.\n", "\n", "For the prediction method you must write an algorithm that takes as input a batch of data and computes the output of the neural net on each input point given in the batch.\n", "\n", "The data batch is given as an $n \\times d$ matrix $X$, where each row is a data point.\n", "\n", "\n", "A neural net as considered here requires two sets of weights and biases\n", "* The weights that map the input data to the input to hidden units. Call that W_1. The bias weights for this we name $b_1$.\n", "\n", "* The weights that map the output of the hidden units to the output. Call that W_2. The bias weights for this we name $b_2$.\n", "\n", "We organize the weighs in matrices $(W_1, W_2)$ and vectors $(b_1,b_2)$ as follows:\n", "* The $i'th$ column of $W_1$ are the weights we multiply with the input data to get the input hidden node $i$. The shape of the $W_1$ matrix is $d \\times h$\n", "* The bias $b_1$ is a vector of size h, the i'th entry the bias to hidden neuron $i$.\n", "* The $i'th$ column of $W_2$ are the weights we multiply with the hidden layer activations to get the input to the i'th output node. $W_2$ is a $h \\times \\textrm{output_size}$ matrix ($h \\times 1$ matrix in our case)\n", "* The bias $b_2$ is a vector of size output_size \n", "\n", "**Task:** In the cell below (partially) complete the neural net class\n", "- Implement the predict function of the neural net\n", "- Implement the score function (least squares $\\frac{1}{n} \\sum_i (\\textrm{nn}(x_i) - y_i)^2$\n", "\n", "Tests:\n", "- We have a simple test case with random weights. The actual error here is random since we just set random weights.\n", "- The second test case uses the weight of a pretreined network for house pricing. Here you should get a score around 0.32 (remove comment to run it)\n"]}, {"cell_type": "code", "execution_count": 31, "metadata": {}, "outputs": [], "source": ["%matplotlib inline\n", "import numpy as np\n", "\n", "class NN():\n", "    \n", "    def __init__(self, input_dim, hidden_size):\n", "        output_size = 1\n", "        self.W1 = np.random.rand(input_dim, hidden_size)\n", "        self.b1 = np.random.rand(1, hidden_size)\n", "        self.W2 = np.random.rand(hidden_size, output_size)\n", "        self.b2 = np.random.rand(1, output_size)\n", "        print('Neural net initialized with random values')\n", "        \n", "    def predict(self, X):    \n", "        \"\"\" Evaluate the network on given data batch \n", "        \n", "        np.maximum may come in handy\n", "        \n", "        Args:\n", "        X: np.array shape (n, d)  Each row is a data point\n", "        \n", "        Output:\n", "        pred: np.array shape (n, 1) output of network on each input point\n", "        \"\"\"\n", "        # compute the following values\n", "        pred = None # the output of neural net n x 1\n", "    \n", "        ### YOUR CODE HERE\n", "        ### END CODE\n", "        return pred\n", "    \n", "    \n", "    def score(self, X, y):\n", "        \"\"\" Compute least squares loss (1/n sum (nn(x_i) - y_i)^2)\n", "        \n", "          X: np.array shape (n, d) - Data\n", "          y: np.array shape (n, 1) - Targets\n", "\n", "        \"\"\"\n", "        score = None\n", "        ### YOUR CODE HERE\n", "        ### END CODE\n", "        return score\n", "        \n", "# random data test\n", "def simple_test():\n", "    input_dim = 3\n", "    hidden_size = 8\n", "    X = np.random.rand(10, input_dim)\n", "    y = np.random.rand(10, 1)\n", "    my_net = NN(input_dim=input_dim, hidden_size=hidden_size)\n", "\n", "    nn_out = my_net.predict(X)\n", "    print('shape of nn_out', nn_out.shape) # should be n x 1\n", "    print('least squares error: ', my_net.score(X, y))\n", "    \n", "# actual data test\n", "def housing_test():\n", "    from sklearn.preprocessing import StandardScaler\n", "    from  sklearn.datasets import fetch_california_housing\n", "    rdata = fetch_california_housing()\n", "    s = StandardScaler()\n", "    Xr = rdata.data\n", "    yr = rdata.target   \n", "    print('data size:', len(yr), 'num features:', Xr.shape[1])\n", "    s.fit(Xr)\n", "    X_scaled = s.transform(Xr)\n", "    house_net = NN(input_dim=Xr.shape[1], hidden_size=8)\n", "    weights = np.load('good_weights.npz')\n", "    house_net.W1 = weights['W1']\n", "    house_net.W2 = weights['W2']\n", "    house_net.b1 = weights['b1'].reshape(1, -1)\n", "    house_net.b2 = weights['b2'].reshape(1, -1)\n", "    print('hidden layer size:', house_net.W1.shape[1])\n", "    lsq = house_net.score(X_scaled, yr.reshape(-1, 1))\n", "    pred = house_net.predict(X_scaled)\n", "    print('mean house price least squares error:', lsq)\n", "    #print('5 house prediction:\\nestimated price , true price')\n", "    #print(np.c_[house_net.predict(X_scaled[0:5, :]), yr[0:5]])\n", "\n", "simple_test()\n", "housing_test()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Exercise 3: Implementing AdaBoost\n", "In this exercise your task is to implement adaboost as described in the lecture and the Boosting note.\n", "We have provided starter code in adaboost.py. See the boosting note for a description of the algorithm.\n", "\n", "You must implement the methods\n", "- ensemble_output\n", "- exp_loss\n", "- predict\n", "- score\n", "- fit\n", "\n", "in that order\n", "\n", "To test your implementation, run adaboost.py\n", "\n", "You should get a final accuracy of around 0.886\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Exercise 4: Gradient Boosting by Hand\n", "In this exercise you must complete one step of gradient boosting with exponential loss on a small data set (X, y) as shown below.\n", "\n", "$X = [1,2,5,3,4]$\n", "\n", "$y = [1,1,1,-1, -1]$\n", "\n", "Asume that we initialize our ensemble model with the constant function $h(x) = 1$\n", "\n", "\n", "**Your task requires the following three steps** \n", "1. Compute the residuals the regression tree should fit (with least squares)\n", "2. Construct the Regression Stump found by fitting the negative gradient\n", "3. Optimize the leaf values such that the newly added tree minimize the exponential loss, with the condition that the number the leaf returns is in the interval [-1, 1].\n", "   What happens if we do not have this constraint that the leaf must return values in [-1, 1]?\n", "\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Exercise 5: Implementing Forward Stagewise Additive Modelling\n", "In this exercise your task is to implement a standard Forward Stategewise Additive Modelling algorithm for regression using Least Squares Loss.\n", "We have provided starter code in **sfam.py**.\n", "\n", "See Elements of statistical learning for a description of the algorithm.\n", "We note that in the iteration each step computes the best new hypothesis h and best new scalar $\\beta$ such that $\\beta h(x)$ is to be added to the ensemble.\n", "We implement this simply by first finding $h$ with the weak learner by fitting the residuals and then computing the optimal constant $\\beta$ for the $h$ found by our weak learner.\n", "\n", "\n", "You must implement the methods\n", "- predict\n", "- score\n", "- fit\n", "\n", "in that order.\n", "\n", "\n", "Notice that fit gets two sets of data and labels X, y and X_val, y_val.\n", "The latter X_val, y_val is a separate validation test set you must test your current ensemble on in each iteration so we can plot the development on data not used for training (where we know the error will always decrease).\n", "\n", "To test your implementation, run sfam.py -max_depth 1\n", "\n", "You can procide different max_depth of the base learner which is a Regression Tree (1 is default).\n", "\n", "With a default base learner with max depth 1 the mean least squares error on both training and test data should be around 0.35. \n", "If you change random state then the results may be different.\n", "\n", "If you increase the max_depth the results will change.  Try for instance max_depth 3 and 5 as well. What do you see?\n", "\n", "\n", "\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.4"}}, "nbformat": 4, "nbformat_minor": 2}