{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Week 8 Exercises\n", "Exercise 4 may take a long time for the computation so maybe wait with this until you completed 5 and 6"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Ex 1: Hinge Loss - Cost and Gradient\n", "In class we talked about that we could rewrite the primal problem (with no kernels) into an unconstrained minimization problem using hinge loss.\n", "For a hypothesis $h$, a data point $x$ with label $y \\in \\{-1, +1\\}$ the hinge loss is defined as\n", "$$\n", "e(h(x), y) = \\max(0, 1-h(x)y)\n", "$$\n", "Define the (average regularized) hinge loss  over the data set as\n", "$$\n", " \\frac{1}{n} \\sum_{i=1}^n \\max(0, 1-h(x_i)y_i) + \\lambda \\|w\\|^2 = \\frac{1}{n} \\sum_{i=1}^n \\max(0,1 -(w^\\intercal x_i +b) y_i) + \\lambda \\|w\\|^2\n", "$$\n", "What is the relation between the SVM penalty $C$ and $\\lambda$\n", "\n", "To make a gradient descent algorithm for Hinge Loss for a linear model we need to\n", "compute the cost and the gradient. \n", "\n", "**Your Task:** Write the code for computing the Hinge Loss and the Gradient at a given value $w$ and $\\lambda$ in the hinge_loss method in the cell below. You should derive the math formula for the gradient before you implement it!.\n", "\n", "\n", "\n", "\n", "\n", "(we know max is not differentiable at zero but let us ignore that and say that the derivative is zero at zero, for more  go to wikipedia and look up subgradient).\n"]}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "n = 100\n", "hinge_data_numpy = 2*np.r_[2*np.random.randn(n, 2) - [4., 4.], 2*np.random.randn(n, 2) + [5., 5.]]\n", "hinge_labels_numpy = np.array([-1] * n + [1] * n)\n", "\n", "def hinge_loss(X, y, w, b, C):\n", "    \"\"\" Compute hinge loss on data with given parameters\n", "    \n", "    Args:\n", "        X: np.array shape n,d \n", "        y: np.array shape n, \n", "        w: np.array shape d,\n", "        b: float\n", "        C: float - (lambda in formula) the scalar multiplied to ||w||^2 in the cost        \n", "    Returns:\n", "    output scalar, grad_w np.array shape d, grad_b scalar\n", "    \"\"\"\n", "    loss = 0\n", "    grad_w = np.zeros(w.shape)\n", "    grad_b = 0\n", "    # - compute loss, reg_loss, grad_w, grad_b\n", "### YOUR CODE HERE \n", "### END CODE\n", "\n", "    assert grad_w.shape == w.shape    \n", "    return loss, grad_w, grad_b\n", "\n", "w = np.array([-1, -1])\n", "b = 1.0\n", "c = 1.0\n", "reg_loss, grad_w, grad_b = hinge_loss(hinge_data_numpy, hinge_labels_numpy, w, b, c)\n", "print('hinge loss: ', reg_loss, 'hinge grad_w: ', grad_w, 'hinge grad_b:', grad_b)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Ex 2: Hinge Loss Gradient Descent with Pytorch\n", "In this exercise we will see how to use pytorch build in optimizer class to do gradient descent for Hinge Loss.\n", "1. Implement a Hinge Loss Classifier trained by gradient descent in Pytorch by completing the methods in the cell below.\n", "2. Test your your implementation from this and the previous exercise by checking that the cost and gradient are the same for both implmentations. In particular check that the gradient from above matches the gradient produced by pytorch.\n", "3. Test that the pytorch gradient descent finds the minimum (fast)"]}, {"cell_type": "code", "execution_count": 10, "metadata": {"scrolled": false}, "outputs": [], "source": ["%matplotlib inline\n", "import matplotlib.pyplot as plt\n", "import torch\n", "import torch.optim as optim\n", "import numpy as np\n", "\n", "\n", "class HingeLossClassifier():\n", "    \n", "    def __init__(self):\n", "        self.w = None\n", "        self.b = None\n", "        \n", "    def hinge_loss(self, X, y, w, b, c):\n", "        \"\"\"\n", "        Compute Hinge Loss on Torch tensors\n", "        torch.sum and x.clamp may com in usefull\n", "     \n", "        Args:\n", "            X: torch.tensor shape n,d \n", "            y: torch.tensor shape n, \n", "            w: torch.tensor shape d,\n", "            b: float\n", "            c: float scaler for c ||w||^2 in loss\n", "        Returns:\n", "        hinge loss cost torch.tensor 1 x 1     \n", "        \"\"\"\n", "        ### YOUR CODE HERE\n", "        ### END CODE\n", "        return out\n", "\n", "    def predict(self, X):\n", "        \"\"\" Compute predictions on X (predictions in -1,+1)\n", "        \n", "        Args:\n", "            X: torch.tensor shape n,d \n", "        \"\"\"\n", "        out = None\n", "        ### YOUR CODE HERE\n", "        ### END CODE\n", "        return out\n", "    \n", "    def score(self, X, y):\n", "        \"\"\" Compute accuracy of classifier on y\n", "        \n", "        Args:\n", "            X: torch.tensor shape n,d \n", "            y: torch.tensor shape n, \n", "        \"\"\"\n", "        acc = None\n", "        ### YOUR CODE HERE\n", "        ### END CODE\n", "        return acc\n", "    \n", "    def fit(self, X, y, c, epochs, lr):\n", "        \"\"\" Gradient Descent Algorithm \n", "        \n", "        Args:\n", "            X: torch.tensor shape n,d \n", "            y: torch.tensor shape n, \n", "            c: float scaler for c ||w||^2 in loss\n", "            epochs: int - number of iterations of gradient descent\n", "            lr: float, learning rate for gradient descent\n", "        \"\"\"\n", "        w = torch.zeros(X.shape[1], requires_grad=True)\n", "        b = torch.zeros(1, 1, requires_grad=True)\n", "        optimizer = optim.SGD([w, b], lr=lr)\n", "        # remember to call optimizer.zero_grad\n", "        ### YOUR CODE HERE\n", "        ### END CODE\n", "        self.w = w.detach()\n", "        self.b = b.detach()\n", "        \n", "hinge_data_torch = torch.from_numpy(hinge_data_numpy).float()\n", "hinge_labels_torch = torch.from_numpy(hinge_labels_numpy).float()\n", "w_test = torch.tensor([-1., -1], requires_grad=True)\n", "b_test = torch.tensor([1.0], requires_grad=True)\n", "c_test = 1.0\n", "\n", "print('Check your computations from above')\n", "cl = HingeLossClassifier()\n", "loss = cl.hinge_loss(hinge_data_torch, hinge_labels_torch, w_test, b_test, c_test)\n", "loss.backward()\n", "print('check loss and gradient: ', loss.item(),'w grad: ', w_test.grad, 'b grad:', b_test.grad)\n", "\n", "print('Run Gradient Descent')\n", "cl.fit(hinge_data_torch, hinge_labels_torch, 1, 30, 0.1)\n", "w = cl.w.numpy()\n", "b = cl.b.numpy().ravel()[0]\n", "print('Learned w and b', cl.w.numpy(), cl.b.numpy())\n", "\n", "def plot_hyperplane(ax, w, b, *args, **kwargs): \n", "    xmin, xmax, ymin, ymax = ax.axis()\n", "    \n", "    if w[1]==0:\n", "        # Vertical line\n", "        print('vert')\n", "        x = np.array((1 / w[0], 1 / w[0]))\n", "        y = np.array((ymin, ymax))\n", "    else:\n", "        x = np.array((xmin, xmax))\n", "        y = (-b -w[0] * x) / w[1]       \n", "    # plot the line\n", "    print(x, y)\n", "    ax.plot(x, y, *args, **kwargs)\n", "\n", "\n", "fig, ax = plt.subplots(1, 1, figsize=(20, 16))\n", "ax.scatter(hinge_data_numpy[:,0], hinge_data_numpy[:, 1], c=hinge_labels_numpy, s=30)\n", "margin = 1/np.linalg.norm(w)\n", "plot_hyperplane(ax, w, b - 1,'k--', linewidth=2)\n", "plot_hyperplane(ax, w, b + 1,'k--', linewidth=2)\n", "plot_hyperplane(ax, w, b,'g-', linewidth=2)\n", "\n", "plt.show()\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Ex 3: Kernel Perceptron \n", "In this exercise you must implement Kernel Perceptron for learning from a stream of data.\n", "The learning algorithm will only do one pass over the data like in a system processing an input data stream.\n", "\n", "The important part is how to actually add Kernels to the Perceptron learning algorithm.\n", "\n", "First we need to represent the hyperplane in the feature space induced by the Kernel.\n", "This must be implemented in the class *Representer* as described below.\n", "\n", "Remeber the Perceptron algorithm, the current solution hyperplane is updated on a mispredicted data point $(x, y)$ as\n", "$$\n", "w = w + y x\n", "$$\n", "In this exercise the hyperplane exists in feature space and must be updated there but in the same way\n", "with one difference. We include a learning rate $\\alpha>0$ that scales the update.\n", "The update that must be implemente  becomes\n", "$$\n", "w = w + \\alpha y \\phi(x)\n", "$$\n", "where $\\phi$ is the feature transform corresponding to the used Kernel. As we will see we do not really need to know what $\\phi$ is.\n", "\n", "This means that the hyperplane solution is a linear combination of (transformed) inputs points and thus can be written as\n", "$$\n", "w = \\sum_i \\alpha_i \\phi(x_i)\n", "$$\n", "and may be represented by storing the list of $\\alpha_i$ and $x_i$. Note we have not discussed how to initialize $w$, which can be done just like an update. Also note that we do not use a bias variable in this exercise.\n", "\n", "\n", "**Task:** In the class Representer implement \n", "* update(x, $\\alpha$): (add point x with weight $\\alpha$ to the hyperplane\n", "* dot(z): compute and return \n", "$$\n", "\\langle w, \\phi(z) \\rangle = \\left \\langle \\sum_i  \\alpha_i \\phi(x_i), \\phi(z) \\right \\rangle = \\sum_i \\alpha_i K(x_i, z)\n", "$$ \n", "(note the indexing here is not over the data set but the set of weight and points comprising w - and we assume the lists are non-empty)\n", "\n", "After you have implemented the representation of the hyperplane you must implement\n", "The Pereptron Classifier in the *KernelPerceptron* class.\n", "* Implement the score function (compute accuracy of classifier on given data X with labels y)\n", "* Implement the fit method - do one scan over the data and for each misprediction (x, y) update $w$ by adding $\\alpha y \\phi(x)$\n", "\n", "Test your implementation by running the cell."]}, {"cell_type": "code", "execution_count": 27, "metadata": {"scrolled": false}, "outputs": [], "source": ["# streaming perceptron with kernels\n", "%matplotlib inline\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "\n", "def load_data():\n", "    \"\"\" Simple helper function for downloading and loading data \"\"\"\n", "    filename = 'nonlinear_data.npz'\n", "    D = np.load(filename)\n", "    return D\n", "\n", "def visualize_kernel_perceptron(X, Y, w):\n", "    \"\"\" Helper function for visualizing decision boundary in input space\"\"\"\n", "    fig, ax = plt.subplots(figsize=(20, 16))\n", "    ax.scatter(X[:, 0], X[:,1 ], c=Y, cmap=plt.cm.Paired, s=20)\n", "    nsize = 50\n", "    xs = ys = np.linspace(-1, 1, nsize)\n", "    xm, ym = np.meshgrid(xs, ys)\n", "    img = np.zeros((nsize, nsize)) # makes a 100 x 100 2d array\n", "    for i, zy in enumerate(ys):\n", "        for j, zx in enumerate(xs):    \n", "            point = np.array([zx, zy])\n", "            predict = w.dot(point)\n", "            img[i, j] = predict\n", "    ax.contour(xs, ys, img, [0], colors='r', linewidths=3)\n", "    plt.show()\n", "\n", "def get_rbf_kernel(gamma=1.0):\n", "    assert gamma > 0, 'Gamma must be positive'\n", "    def K(x, z):\n", "        return np.exp((-gamma * np.sum((x-z)**2)))\n", "    return K\n", "\n", "class Representer():\n", "    \"\"\" Represents a hyperplane H in Feature space that is a linear combination of transformed points from input space\n", "        \n", "        The class can evaluate an input in the original input space mapped to H against the hyperplane        \n", "    \"\"\"\n", "    def __init__(self, K):\n", "        self.dat = list()\n", "        self.alpha = list()\n", "        self.K = K\n", "\n", "    def update(self, x, a):\n", "        \"\"\" Update hyperplane Representer by point x with weight a i.e. w = w + a * x\n", "        \n", "        Args:\n", "            x: np.array - data point\n", "            a: float - data weight\n", "        \"\"\"\n", "        ### YOUR CODE HERE\n", "        ### END CODE\n", "\n", "    def dot(self, z):\n", "        \"\"\" Compute inner product between hyperplane and z in feature (kernel) space\n", "                \n", "        Args:\n", "            z: np.array\n", "        \"\"\"\n", "        ### YOUR CODE HERE\n", "        ### END CODE\n", "        return out\n", "    \n", "class KernelPerceptron():\n", "    \n", "    def __init__(self, K):\n", "        self.K = K\n", "        self.w = None\n", "    \n", "    def fit(self, X, Y, K, alpha=0.1):\n", "        \"\"\" Kernel Perceptron Algorithm \n", "            1. Do one pass over the data - for each misprediction (w.dot(x) * y <= 0) add phi(x) with weight y * alpha to hyperplane representation\n", "            \n", "        \"\"\"\n", "        w = Representer(self.K)\n", "        ### YOUR CODE HERE\n", "        ### END CODE\n", "        \n", "    def score(self, X, Y):\n", "        \"\"\" Compute Classifier Accuracy\n", "        Args:\n", "          X: np.array n, d\n", "          Y: np.array n, \n", "        \n", "        Returns:\n", "        out: scalar - accuracy of model on data X with labels Y\n", "        \"\"\"\n", "        out = None\n", "        ### YOUR CODE HERE\n", "        ### END CODE\n", "        return out\n", "            \n", "    \n", "\n", "D = load_data()\n", "X = D['X4']\n", "Y = D['y4']\n", "rbf_kernel = get_rbf_kernel(1.0)\n", "kernel_perceptron_classifier = KernelPerceptron(rbf_kernel)\n", "kernel_perceptron_classifier.fit(X, Y, rbf_kernel)\n", "print('In Sample Accuracy after one scan: {0}'.format(kernel_perceptron_classifier.score(X, Y)))\n", "visualize_kernel_perceptron(X, Y, kernel_perceptron_classifier.w)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Ex 4: Deep Learning with Pytorch \n", "\n", "We have seen all the tools needed to do deep learning. In this exercise we will compare small convolutional neural nets to feed forward networks on a more trying data set of images, known as CIFAR 10. \n", "We have shown some example data from CIFAR 10 in the next cell. Remember to set your own data path.\n", "In the cell after this we show how use the neural net package from pytorch to construct neural networks, including convolutional neural nets.\n", "\n", "Your task is to make a standard feed forward network and a convolutional neural network and compare them on CIFAR data.\n", "We will only train for a few epochs because these networks are quite computational expensive.\n", "\n", "We have written all the code for getting the data and fitting the network. All you need to do is implement a standard feed forward network and a convolutinal network and compare them.\n", "\n", "**Note that training convolutional neural networks on large data sets is very time consuming and it may be worthwhile to skip this exercise and move on to exercise 5 and 6 before you try and solve this one**\n", "\n", "See **conv_cifar.py** for starter code\n", "\n"]}, {"cell_type": "code", "execution_count": 25, "metadata": {"scrolled": false}, "outputs": [], "source": ["import torch\n", "import torch.nn as nn\n", "import torch.optim as optim\n", "import torch.nn.functional as F\n", "from torch.utils.data import DataLoader\n", "import torchvision.datasets as datasets\n", "import torchvision\n", "import torchvision.transforms as transforms\n", "\n", "\n", "\n", "## LOAD DATA\n", "transform = transforms.Compose(\n", "    [transforms.ToTensor(),\n", "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n", "\n", "\n", "data_path = './torch_data/'\n", "### YOUR CODE HERE\n", "### END CODE\n", "#set your own data path\n", "_trainset = torchvision.datasets.CIFAR10(root=data_path, train=True,\n", "                                        download=True, transform=transform)\n", "_train_loader = torch.utils.data.DataLoader(_trainset, batch_size=32,\n", "                                          shuffle=True, num_workers=4)\n", "\n", "_testset = torchvision.datasets.CIFAR10(root=data_path, train=False,\n", "                                       download=True, transform=transform)\n", "_test_loader = torch.utils.data.DataLoader(_testset, batch_size=32,\n", "                                         shuffle=False, num_workers=4)\n", "\n", "classes = ('plane', 'car', 'bird', 'cat',\n", "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n", "\n", "\n", "print('data shape', _trainset[0][0].shape)\n", "print('data type', type(_trainset[0][0]))\n", "\n", "fig, axes = plt.subplots(8, 8, figsize=(20,16))\n", "for i, ax in enumerate(axes.ravel()):\n", "    tmp = (_trainset[i][0].permute(1, 2, 0)+1)/2\n", "    ax.imshow(tmp)\n", "    ax.axis('off')\n", "    data_class = classes[_trainset[i][1]]\n", "    ax.set_title(data_class)\n", "plt.show()\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["## A Convolutional Neural Net Class\n", "class ConvNet(nn.Module):\n", "    def __init__(self):\n", "        super(ConvNet, self).__init__()\n", "        self.conv1 = nn.Conv2d(3, 5, kernel_size=3, padding=1) #conv operator from 3 channels to 5 with 3x3 filters withh padding (image size unchanged)\n", "        self.pool = nn.MaxPool2d(2, 2)\n", "        self.conv2 = nn.Conv2d(5, 7, kernel_size=3, padding=1) #conv operator from 5 channels to 7 with 3x3 filters withh padding (image size unchanged)\n", "        self.fc = nn.Linear(8 * 8 * 7 , 10) # standard linear layer with 10 outputs for softmax\n", "\n", "    def forward(self, dat):\n", "        \"\"\" Implement the forward pass\n", "            \n", "            Args:\n", "            dat: torch.tensor shape (mini_batch_size, channels, width, height) which is (-1, 3, 32, 32)\n", "            \n", "            return x, torch.tensor shape (mini_batch_size, output_size)\n", "        \"\"\"\n", "        x = F.relu(self.conv1(dat)) # apply conv operator and then relu on top\n", "        x = self.pool(x) # apply pooling operator\n", "        x = F.dropout(x, p=0.5) # apply dropout - for the fun of it\n", "        x = F.relu(self.conv2(x)) # apply another round of convolutions and relu\n", "        x = self.pool(x) # pool again\n", "        x = x.view(-1, 8 * 8 * 7) # a reshape operation that flattens the remaing image into a vector\n", "        x = self.fc(x) # apply feed forward layer \n", "        return x\n", "\n", "\n", "conv_net = ConvNet()\n", "conv_net(_trainset[0][0].reshape(1,3,32,32))\n", "\n", "def test_accuracy(neural_net, data_loader):\n", "    correct = 0\n", "    total = 0\n", "    with torch.no_grad():\n", "        for data in data_loader:\n", "            images, labels = data\n", "            outputs = neural_net(images)\n", "            _, predicted = torch.max(outputs.data, 1)\n", "            total += labels.size(0)\n", "            correct += (predicted == labels).sum().item()\n", "            \n", "    acc = 100.0 * correct/total\n", "    return acc\n", "\n", "\n", "def fit(net, train_loader, test_loader, epochs=5):\n", "    criterion = nn.CrossEntropyLoss() # The negative log likelihood loss\n", "    lr = 0.1\n", "    optimizer = optim.SGD(net.parameters(), lr=lr)    \n", "    for epoch in range(epochs):  # loop over the dataset multiple times\n", "        running_loss = 0.0\n", "        running_total = 0.0\n", "        for i, data in enumerate(train_loader):\n", "            # get the inputs\n", "            inputs, labels = data\n", "            # zero the parameter gradients\n", "            optimizer.zero_grad()\n", "            # forward \n", "            outputs = net(inputs)\n", "            loss = criterion(outputs, labels)\n", "            # backward step\n", "            loss.backward()\n", "            # update parameters\n", "            optimizer.step()\n", "\n", "            # print running statistics\n", "            running_loss += loss.item()\n", "            running_total += labels.size(0)\n", "            if i % 200 == 199:    # print every 200 mini-batches\n", "                print(f'Epoch {epoch + 1}, minibatch {i + 1}, mean running loss {running_loss/running_total}')\n", "        print(f'epoch {epoch+1} done')\n", "        train_acc = test_accuracy(net, train_loader)\n", "        test_acc = test_accuracy(net, test_loader)\n", "        print('train accuracy:', train_acc)\n", "        print('test accuracy:', test_acc)\n", "    \n", "    print('Finished Training')\n", "fit(conv_net, _train_loader, _test_loader, epochs=1)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Exercise 5: Determining whether PCA is useful / relevant for a given dataset\n", "  \n", "In Principal Components Analysis assume we have computed the variances (eigenvalues of the empirical covariance matrix), $\\lambda_1,\\dots,\\lambda_d>0$.\n", "\n", "Define $\\bar{\\lambda} = \\frac{1}{d} \\sum_{i=1}^d \\lambda_i$, the mean of the variances (eigenvalues).\n", "Now consider the following usefulness measure of PCA:\n", "    \n", "$$\n", "score = \\sum_{i=1}^d (\\lambda_i - \\bar{\\lambda})^2\n", "$$\n", "    \n", "\n", "\n", "Discuss whether $score$ could be used as a measure of whether performing a PCA would be useful. What is $score$ and what does it mean for it to be zero and what would it mean if it is large?\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Exercise 6: PCA on digits\n", "In this exercise we will experiment with PCA on digits.\n", "\n", "1. Run PCA on the data (first 5000 digits) http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html (use n_components = 784). Save this PCA model in a variable to use in the subsequent exercises. Use plot_images to plot the 20 directions with largest variance (the directions can be found in the attribute ``components_``). \n", "\n", "\n", "2. Consider the proportion of variance explained by each principal component (see ``explain_variance_``, and ``explained_variance_ratio_``). Make a plot of the variance explained as a function of the number of principal components, i.e. x axis is number of components, $k=1, \\dots, 784$  and y axis is proportion of variance explained by the first $k$ principal components in total (``np.cumsum()`` may be useful). Looking at this plot, how many components do we approximately need to get 90 percent of the variance explained?\n", "\n", "\n", "3. Take the first 20 data points from the data and project onto the first $k$ components for $k$ in $[1, 2,4,8,16, 32, 64]$ and plot them as images. What do you see?\n", "\n", "\n", "4. Map all the data to 2D (use only 2 principal components) and make a scatter plot where you color with the label $y \\in \\{0, 1, \\dots, 9 \\}$ and see if there is some structure. It may be helpful to use `plt.scatter` with `cmap = plt.cm.Paired`, like (`ax.scatter(x,y, c=lab, cmap=plt.cm.Paired`). \n", "    \n", "    \n", "5. Map all data to 32 Dimension and run a Classifier and output in sample accuracy. (How should you select target dimension in real life?). You can use any classifier you want (we have already imported a support vector machines and an SGD classifier for you, e.g. use `model = SGDClassifier(loss=\"hinge\", alpha=0.01, max_iter=200, fit_intercept=True)`. Compare with the result and runtime you get when using the 'uncompressed' data set.\n"]}, {"cell_type": "code", "execution_count": 28, "metadata": {}, "outputs": [], "source": ["%matplotlib inline \n", "import matplotlib.pyplot as plt\n", "import numpy as np\n", "import os\n", "from sklearn.decomposition import PCA\n", "from sklearn.linear_model import SGDClassifier\n", "from sklearn.svm import SVC\n", "import torchvision.datasets as datasets\n", "\n", "def plot_images(dat, k=20, size=28):\n", "    \"\"\" Plot the first k vectors as 28 x 28 images \"\"\"\n", "    x2 = dat[0:k,:].reshape(-1, size, size)\n", "    x2 = x2.transpose(1, 0, 2)\n", "    fig, ax = plt.subplots(figsize=(20,12))\n", "    ax.imshow(x2.reshape(size, -1), cmap='bone')\n", "    ax.set_yticks([])\n", "    ax.set_xticks([])\n", "    plt.show()\n", "\n", "mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=None)\n", "# Load full dataset\n", "X = mnist_trainset.train_data.numpy().reshape((60000, 28*28))\n", "y = mnist_trainset.train_labels.numpy()\n", "\n", "# Take out random subset\n", "rp = np.random.permutation(len(y))\n", "X = X[rp[:5000], :]\n", "y = y[rp[:5000]]\n", "\n", "### YOUR CODE HERE\n", "### END CODE\n", "\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.4"}}, "nbformat": 4, "nbformat_minor": 2}