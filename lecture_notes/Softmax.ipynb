{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax  Regression - Logistic Regression for many classes\n",
    "\n",
    "In this notebook we generalize logistic regression to handle $K$\n",
    "classes instead of 2. This is often known as Multinomial Regression or Softmax, but sometimes Logistic Regression refers to this more general case.\n",
    "\n",
    "**We will use the exact same approach as for Logistic Regression**, but it becomes slightly more technical due to the extra\n",
    "classes.\n",
    "\n",
    "## The Setup\n",
    "Assume the problem at hand is a multiclass classification problem with $K$ classes.\n",
    "For this problem we choose to encode the target labels in a very particular way.\n",
    "\n",
    "A target value $y$ is naturally represented as a number in $\\{1, \\dots, K\\}$. However, for this exposition we chose to represent it as a vector of\n",
    "length $K$ with all zeros except one which corresponds to the class. This is called a *one-in-K encoding*.\n",
    "\n",
    "If a data point $x$ is labelled with class 3 and there are five classes then\n",
    "$y = [0,0,1,0,0]^\\intercal = e_3$.\n",
    "\n",
    "To store the labels of all data points we create a matrix $Y$ of size $n \\times K$.\n",
    "The data matrix $X$ is unchanged.\n",
    "\n",
    "\n",
    "$$\n",
    "X=\\begin{pmatrix} \n",
    "1&- & x_1^T & - \\\\\n",
    "\\vdots & \\vdots & \\vdots \\\\\n",
    "1&- & x_n^T & - \\\\\n",
    "\\end{pmatrix}\\in \\mathbb{R}^{n \\times d}\\quad\\quad \n",
    "y=\\begin{pmatrix}\n",
    "- & y_1^T & -\\\\\n",
    "- & \\vdots &- \\\\\n",
    "- & y_n^T & -\\end{pmatrix}\\in\\{0,1\\}^{n\\times K}\n",
    "$$\n",
    "\n",
    "\n",
    "To generalize to $K$ classes we will use $K$ weight vectors $w_1,\\dots,w_k$ each of length $d$, one for each class.\n",
    "Note this fits with the original wine example from the lectures.\n",
    "Also, to use  such a list of weight vectors for classification we do as described in the wine example:\n",
    "\n",
    "Given data x, compute $w_i^\\intercal x$ for $i=1,\\dots, K$ and return the index of the largest value.\n",
    "\n",
    "Now, this list of weight vectors we pack into a matrix $W$ of size $d \\times K$ by putting $w_1$ in column one and so on.\n",
    "$$\n",
    "W=\\begin{pmatrix} \n",
    "(w_1)_1  & \\dots & (w_K)_1 \\\\\n",
    "\\vdots & \\vdots & \\vdots \\\\\n",
    "(w_1)_d  & \\dots & (w_K)_d \\\\\n",
    "\\end{pmatrix}\\in \\mathbb{R}^{d \\times n}\n",
    "$$\n",
    "\n",
    "This way we can compute the weighed sum for each class by the vector matrix product $x^\\intercal W$ and then pick argmax of that to do the classification. Pretty Neat!.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape w: (10, 3)\n",
      "Shape x: (10, 1)\n",
      "model (unnormalzed log) predictions: - picke the larger one\n",
      " [[33.27812034 33.97847283 26.81526456]]\n",
      "class output:  2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# example with 3 classes and d = 10\n",
    "W = np.random.rand(10, 3)\n",
    "print('Shape w:', W.shape)\n",
    "x = np.array([1., 2., 3., 4., 5., 6., 7., 8., 9., 10.0]).reshape(10, 1)\n",
    "print('Shape x:', x.shape)\n",
    "model_predictions = x.T @ W\n",
    "print('model (unnormalzed log) predictions: - pick the larger one\\n', model_predictions)\n",
    "print('class output: ', 1 +np.argmax(model_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic Outputs\n",
    "However, as for Logistic Regression we are really interested in computing a probability of each class given a data point.\n",
    "Said differently, given a set of model parameters $W$ and a data point $x$ we want $P(y=i\\mid x, W)$ for $i=1,\\dots K$.\n",
    "This is a list of length $K$ given the probability of each class as estimated by the model.\n",
    "\n",
    "So how to define this probabilistic model.  \n",
    "We need to map the output of $w_1^\\intercal x, \\dots, w_K^\\intercal x= x^\\intercal W$ which is a vector of $K$ real numbers to a vector of $K$ non-negative numbers that sum to one, generalizing the sigmoid/logistic function.\n",
    "\n",
    "This generalization is the **softmax** function.\n",
    "Softmax takes as input a vector of length $K$\n",
    "and outputs another vector of the same length $K$,\n",
    "that is a mapping from the $K$ input numbers into $K$\n",
    "*probabilities*, e.g. non-negative numbers that sum to one.  Softmax is defined as\n",
    "\n",
    "$$\n",
    "\\textrm{softmax}(x)_j =\n",
    "\\frac{e^{x_j}}\n",
    "{\\sum_{i=1}^K e^{x_i}}\\quad\n",
    "\\textrm{ for }\\quad j = 1, \\dots, K.\n",
    "$$\n",
    "where  $\\textrm{softmax}(x)_j$ denote the $j$'th entry in the vector.\n",
    "\n",
    "Notice that the denominator acts as a normalization term that ensures\n",
    "that the probabilities sum to one and that the exponentiaion ensures all numbers are positive. As for the sigmoid function we get nice derivatives, (exercise later)\n",
    "\n",
    "$$\n",
    "\\frac\n",
    "{\\partial \\;\\textrm{softmax}(x)_i}\n",
    "{\\partial x_j} =\n",
    "(\\delta_{i,j} - \\textrm{softmax}(x)_j)\n",
    "\\textrm{softmax}(x)_i\\quad\\quad\\text{where}\\quad\\quad\n",
    "\\delta_{ij}=\\begin{cases}1 &\\text{if }i=j\\\\\n",
    "0 & \\text{else}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "With this softmax defined we can now define our probabilistic model\n",
    "$$\n",
    "p(y \\mid x, W) =\n",
    "\\textrm{softmax}(x^\\intercal W) =\n",
    " \\left \\{\n",
    "\\begin{array}{l l}\n",
    " \\textrm{softmax}(x^\\intercal W)_1 & \\text{ if } y = e_1,  \\\\\n",
    " \\vdots & \\\\\n",
    " \\textrm{softmax}(x^\\intercal W)_K & \\text { if } y = e_K.\n",
    "\\end{array}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "Think of the probability distribution over $y$ as throwing a $K$-sided die\n",
    "where the likelihood of landing on each of the $K$ sides is stored in the\n",
    "vector $\\textrm{softmax}(W^\\intercal x)$ (which is a vector of length $K$) and the\n",
    "probability of landing on side $i$ is $\\textrm{softmax}(W^\\intercal x)_i$. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax of the ones vector:  [0.33333333 0.33333333 0.33333333]\n",
      "That seems reasonable, right?\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.array([1, 1, 1])\n",
    "softmax = np.exp(x)/np.sum(np.exp(x))\n",
    "print('softmax of the ones vector: ', softmax)\n",
    "print('That seems reasonable, right?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for logistic regression we compute the likelihood of the data given a fixed matrix of parameters.\n",
    "We use the notation $[z]$ for the indicator function e.g. $[z]$ is one if $z$ is true and zero otherwise.\n",
    "\n",
    "$$\n",
    "P(D \\mid W) =\n",
    "\\prod_{(x,y)\\in D}\n",
    "\\prod_{j=1}^K\n",
    "\\textrm{softmax}(x^\\intercal W)_j^{[y_j=1]}\n",
    "=\n",
    "\\prod_{(x,y)\\in D}\n",
    "y^\\intercal\n",
    "\\textrm{softmax}(x^\\intercal W)\n",
    ".\n",
    "$$\n",
    "\n",
    "This way of expressing is the same as we did for logistic regression.\n",
    "The product over the $K$ classes will have one element\n",
    "that is not one, namely the $y_j$'th element ($y$ is a\n",
    "vector of $K-1$ zeros and a one). The remaining probabilities are\n",
    "raised to a power of zero and has the value one.\n",
    "\n",
    "## The Negative Log Likelihood\n",
    "For convenience we minimize the negative log likelihood of the data\n",
    "instead of maximizing the likelihood of the data and get a pointwise sum.\n",
    "\n",
    "$$\n",
    "\\begin{align}\\textrm{NLL}(D\\mid W) &=\n",
    "-\\sum_{(x,y)\\in D}\n",
    "\\sum_{j=1}^K\n",
    "[y_j=1]\n",
    "\\ln (\\textrm{softmax}(x^\\intercal W)_j)\n",
    "\\\\\n",
    "&=-\\sum_{(x,y)\\in D}\n",
    "y^\\intercal\n",
    "\\ln (\\textrm{softmax}(x^\\intercal W))\n",
    ".\n",
    "\\end{align}\n",
    "$$\n",
    "Notice again that inside the last summation only one value will be nonzero. For a particular data point (x, y) where $y=e_j$ let $z = W^\\intercal x$ be the input to softmax then the cost for that point is just\n",
    "$$ - \\ln \\mathrm{softmax}(z)_j = \\ln \\left( \\frac{e^{z_j}}{\\sum_{i=1}^d e^{z_i}}\\right) = - (z_j - \\ln \\sum_{i=1}^d e^{z_i}) $$\n",
    "\n",
    "\n",
    "Again we define $E_\\textrm{in} = \\frac{1}{|D|} \\textrm{NLL}$, which we need to minimize.  \n",
    "To apply stochastic mini-batch gradient descent as for Logistic Regression all you really need is the\n",
    "gradient of the negative log likelihood function.  This gradient is a\n",
    "*simple* generalization of the one used in logistic\n",
    "regression. There is a set of parameters for each of $K$ classes, $W_j$ for\n",
    "$j=1,\\ldots,K$ (the $j$'th column in the parameter matrix $W$) that must be learned.\n",
    "Luckily some nice people tell you what the gradient is: \n",
    "$$\n",
    "\\nabla \\textrm{NLL}(W) =\n",
    "-X^\\intercal\n",
    "(Y - \\textrm{softmax}(XW)),\n",
    "$$\n",
    "\n",
    "where softmax is taken on each row of the matrix (that is, $X W$ is an\n",
    "$n \\times K$ matrix and softmax is computed for each training case over\n",
    "the $K$ classes).\n",
    "\n",
    "We will skip the derivation of the gradient that can be found by application of the chain rule.\n",
    "\n",
    "\n",
    "## Numerical Issues with Softmax\n",
    "There are some numerical issues with the softmax function\n",
    "\n",
    "$$\n",
    "\\textrm{softmax}(x)_j = \\frac{e^{x_j}}{\\sum_{i=1}^K e^{x_i}} \\textrm{ for } j=1,\\ldots,K.\n",
    "$$\n",
    "\n",
    "This is because this is a sum of exponentials (before taking logs again),\n",
    "and exponentiation of numbers tend to make them very large giving numerical problems.\n",
    "Let's look at the function for a fixed $j$,\n",
    "$\\frac{e^{x_j}}{\\sum_{i=1}^K e^{x_i}}$.\n",
    "Since the logarithm and the exponential function are each other's inverse,\n",
    "we may write it as\n",
    "$$\n",
    "e^{\\textstyle x_j - \\ln(\\sum_{i=1}^K e^{x_i})}\n",
    "$$\n",
    "\n",
    "The problematic part is the logarithm of the sum of exponentials.\n",
    "However, we can move $e^c$ for any constant $c$ outside the sum, that is,\n",
    "$$\n",
    "\\ln\\left(\\sum_i e^{x_i}\\right) =\n",
    "\\ln\\left(e^c \\sum_i e^{x_i-c}\\right) =\n",
    "c + \\ln\\left(\\sum_i e^{x_i -c}\\right).\n",
    "$$\n",
    "\n",
    "We need to find a good $c$, and we choose $c = \\max_i x_i$ since\n",
    "$e^{x_i}$ is the dominant term in the sum. We are less concerned with\n",
    "values being inadvertently rounded to zero since that does not\n",
    "change the value of the sum significantly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
