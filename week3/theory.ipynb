{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Theoretical Exercises week 3\n", "**Like last week, it is very imporant that you try to solve every exercise. \n", "If you do not solve the exercise, focus on understanding the question, and try to figure out what it is you do not understand.**\n", "\n", "Exercise 6 in particular, is pretty technical so do despair if you cannot solve it.\n", "\n", "\n", "The TA's will be very happy to answer questions during the TA session or on the board.\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# 1. Matrix of Derivatives\n", "\n", "In Linear Regression we define the in sample error as (ignoring the normalizing factor 1/n)\n", "\n", "$$E_\\textrm{in}(w) = \\sum_{i=1}^{n} (w^\\intercal x_i - y_i)^2 $$ \n", "\n", "Let $X$ be the the data matrix of shape $n \\times d$ with data point $x_i$ as the $i$'th row. Let $y$ be the label vector of shape $n \\times 1$ with label $y_i$ as the $i$'th entry. Let $w$ be the weight vector of shape $d \\times 1$.  \n", "\n", "$$X=\\begin{pmatrix}\n", "- & x_1^T & - \\\\\n", "- & \\vdots & - \\\\\n", "- & x_n^T & - \\\\\n", "\\end{pmatrix}\\in \\mathbb{R}^{n \\times d}\\quad\\quad\\quad\n", "y=\\begin{pmatrix}\n", "y_1\\\\\n", "\\vdots\\\\\n", "y_n\n", "\\end{pmatrix}\\in \\mathbb{R}^{n \\times 1}$$\n", "\n", "The in-sample error rate $E_{in}$ is then equal to \n", "\n", "$$E_\\textrm{in}(w) = \\sum_{i=1}^{n} (w^\\intercal x_i - y_i)^2 = \\|Xw-y\\|^2 = (Xw-y)^\\intercal (Xw-y)$$\n", "\n", "\n", "In the lecture we proved that for Linear Regression the optimal weight vector $w_\\textrm{lin}$ for minimizing $E_\\textrm{in}$ was $w_\\textrm{lin}=(X^\\intercal X)^{-1} X^\\intercal y$. \n", "\n", "To do this we used facts about the derivatives. \n", "* If we have a function $f(z): \\mathbb{R}^{a} \\rightarrow \\mathbb{R}^b$ such that $f(z) = [f_1(z),\\dots, f_b(z)]$ then the matrix of derivatives $\\frac{\\partial f}{\\partial z}$ is of size $b\\times a$ where\n", "$$ \\left[\\frac{\\partial f}{\\partial z} \\right]_{i,j} = \\frac{\\partial f_i}{\\partial z_j} $$\n", "\n", "For example: Let $f(x): R^2 \\rightarrow R^3$ be the function $f([x_1, x_2]) = [x_1, x_2, x_1 \\cdot x_2]$ then \n", "the matrix of derivatives has shape $3 \\times 2$ and looks like $ \\frac{\\partial f}{\\partial x} =\n", " \\begin{bmatrix}\n", "  1 & 0 \\\\\n", "  0 & 1  \\\\\n", "  x_2  & x_1 \\\\\n", " \\end{bmatrix} $\n", "\n", "In our proof we used the following identities about the matrix of derivatives\n", "\n", "* $f: R^d \\rightarrow R^d, f(z) = Xz-y$, the matrix of derivatives $\\frac{\\partial f}{\\partial x}$ is $X$\n", "* $g: R^d \\rightarrow R, g(z) = z^\\intercal z$, the matrix of derivatives $\\frac{\\partial g}{\\partial x}$ is $2z^\\intercal$\n", "\n", "Notice that $E_\\textrm{in} = g(f(w))$. With these identities, we compute the gradient $\\nabla E_\\text{in}$ by multiplying the matrix of derivativers of $g$, and $f$ evaluated at their inputs (abiding the mighty Chain Ruler), and take the transpose. https://en.wikipedia.org/wiki/Chain_rule\n", "\n", "Since $f(w) = Xw- z$, $g(f(w)) = (Xw-y)^\\intercal (Xw - y)$\n", "Then \n", "\n", "$$\n", "\\nabla E_\\textrm{in} = (2(Xw-y)^\\intercal I X)^\\intercal = 2X^\\intercal(Xw- y)\n", "$$\n", "\n", "Solving for the zero vector gives $w_\\textrm{lin}$ (and the 2 factor becomes irrelevant, like the initial 1/n scaling).\n", "\n", "## Your job is to prove the two identities. \n", "\n", "* Let $f(w) = Xw - y$, Where $X$ is a $n \\times d$ matrix, $y$ is a $n\\times 1$ vector and $w$ is a $w \\times 1$ vector (function from $\\mathbb{R}^d \\rightarrow \\mathbb{R}^n$). \n", "Show that the matrix of derivatives of $f$ is X. \n", "\n", "Hint: You can think of $f = [f_1,\\dots,f_n]$ as n output functions where $f_i(w) = x_{i}^\\intercal w - y_i$ and $x_i$ is the i'th row of $X$ (as a column vector) and $y_i$ is the i'th entry in vector $y$. Start with $\\frac{\\partial f_1}{\\partial w_1}$ to see if a pattern emerges\n", "* Let $g(z) = z^\\intercal z$ where z is a vector (the squared norm of $z$). Show that the matrix of derivatives is $2z^\\intercal$\n", "\n", "\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Exercise 2: Convex Functions \n", "\n", "See https://en.wikipedia.org/wiki/Convex_function for the definitions of convex functions.\n", "At the lectures we saw two definitions. But there is a third one that is usually easier and is as follows.\n", "\n", "Let $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}$ be a function that is twice differentiable.\n", "\n", "Define the Hessian $H$ as the matrix of second order derivatives as (a matrix of shape $n \\times n$):\n", "$$H = \\left[ \\frac{\\partial^2 f}{\\partial x_i \\partial x_j}\\right]_{i,j}$$\n", "\n", "A function f is convex if $H$ is a Positive Semidinite Matrix (PSD) which is the case if $\\forall x\\in \\mathbb{R}^d$,  $ x^\\intercal H x \\geq 0  $ \n", "\n", "For a function $f: \\mathbb{R} \\rightarrow \\mathbb{R}$ the Hessian $H = f''$, and the condition says that $f''(x) \\geq 0 \\forall x$.\n", "\n", "\n", "\n", "\n", "Which of the following functions are convex on ${\\mathbb R}$? \n", "\n", "-   $f(x) = 2$\n", "\n", "-   $f(x) = -\\ln (x), x>0$\n", "\n", "-   $f(x) = x^3$\n", "\n", "-   $f(x) = x^2 + x^4$\n", "\n", "\n", "**hint: use the newly defined definition to determine convexity**"]}, {"cell_type": "code", "execution_count": 4, "metadata": {}, "outputs": [], "source": ["%matplotlib inline\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "# Lets plot them\n", "x = np.linspace(-1, 1, 1000)\n", "xp = x[x>0]\n", "plt.figure(figsize=(10,8))\n", "plt.plot(x, [2 for y in x], 'r-', label='f(x)=2')\n", "plt.plot(xp, [-np.log(z) for z in xp], 'g-', label='f(x)=ln(x), x>0')\n", "plt.plot(x, x**3, 'b-', label='f(x)=x^3')\n", "plt.plot(x, x**2 + x**4, 'y-', label='f(x)=x^2+x^4')\n", "plt.legend()\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Exercise 3: Convexity of Linear Regression Cost Funtion\n", "In this exercise your job is to prove that the cost function for linear regression is convex.\n", "**Prove that the function $E_\\mathrm{in}(w) = \\frac{1}{n} \\|Xw -y\\|^2$ is convex.** \n", "\n", "\n", "\n", "- Hint 1: Prove that the for all matrices $A$, that $A^\\intercal A$ is psd. \n", "- Hint 2: Compute the Hessian of $E_\\mathrm{in}(w)$ \n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Exercise 4: Maximum Likelihood Linear Regression\n", "\n", "The Linear Regression method may also be derived as a maximum likelihood\n", "procedure. In linear regression the function we learn is choosen to\n", "minimize mean squared error, a criterion that we introduced more or less\n", "arbitrarily.\n", "\n", "I.e. Given X, y compute\n", "$$\n", "w_{\\textrm{opt}} = \\textrm{argmin}_w: \\sum_{i=1}^n (w^\\intercal x_i - y_i)^2\n", "$$\n", "We now revisit Linear Regression from the point of view of maximum\n", "likelihood estimation. \n", "\n", "We consider the target function a conditional distribution $p(y\n", "| x)$ and assume it is defined as\n", "$$\n", "p(y\\mid x,w) = w^\\intercal x + \\varepsilon, \n", "$$ \n", "for some unknown $w$, where $\\varepsilon$ is a\n", "noise term independent of $x$ that is normally distributed with zero\n", "mean and variance $\\sigma^2$ i.e. \n", "$$\n", "\\mathbb{E}[\\varepsilon] = 0, \\mathbb{E}[\\varepsilon^2] =\\sigma^2\n", "$$\n", "\n", "For the (1D) normal distribution with mean $\\mu$ and variance $\\sigma^2$\n", "the probability density function is\n", "$$\n", "p(x) = \\frac{1}{\\sqrt{2\\sigma^2\\pi}}e^{-(x-\\mu)^2/2\\sigma^2}\n", "$$\n", "\n", "In other words, given $x$, the target function  outputs a value $y$ that is\n", "distributed as a Gaussian (normal distribution) around $w^\\intercal\n", "x$. \n", "We can now write $p(y\\mid x,w)$ as\n", "$$\n", "p(y \\mid x,w ) = \\frac{1}{\\sqrt{2\\sigma^2\\pi}}e^{-(y-w^\\intercal x)^2/2\\sigma^2}\n", "$$\n", "for some unknown $w$ that we wish to learn. \n", "\n", "\n", "We want to make an algorithm that computes the maximum likelihood\n", "parameters $w_\\textrm{ml}$ of our model. We are given a data set $D\n", "= \\{(x_i, y_i) \\mid i = 1, \\dots, n\\}$ and for a fixed $w$ we let $P(D\n", "\\mid w) = \\prod_{i=1}^n p(y_i \\mid x_i ,w)$ be the likelihood of the\n", "data given $w$. Your job is to derive an algorithm for computing the\n", "maximum likelihood parameters, namely\n", "$w_\\mathrm{ml} = \\operatorname*{arg\\,max}_w\n", "P(D \\mid w)$.\n", "\n", "\n", "**Hint:** Minimize the negative log likelihood of the data instead and note\n", "that we end up with a formula for computing $w$ that should look\n", "familiar.\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Exercise 5: Linear Regression and the missing inverse\n", "In linear regression, given data matrix $X$ and labels vector $y$ the optimal weight vector $w$ (minimizing $\\|Xw-y\\|_2^2$, is found simply by computing the matrix product\n", "$$\n", "(X^\\intercal X)^{-1}X^\\intercal y\n", "$$\n", "That only makes sense if $(X^\\intercal X)$ is in fact invertible.\n", "\n", "In class i suggested that if indeed $(X^\\intercal X)$ is not invertible then we should remove linear dependent columns from $X$.\n", "In this exercise you must argue that this is a good idea.\n", "\n", "To do this, you must prove/argue the two following two things\n", "* Removing linear dependent columns from X does not change the cost of an optimal solution $w$\n", "* If $(X^\\intercal X)$ is not invertible then $X$ contains linear dependent columns\n", "\n", "\n", "\n", "\n", "HINT 1: you can use a well known linear algebra fact that rank(X) = rank($X^\\intercal X$) = rank($X X^\\intercal$)\n", "\n", "HINT 2: $Xw$ is in the column space of $X$\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Exercise 6:  Show that the cost function for Logistic Regression is convex (Hard Exercise)\n", "### Try at least the first part\n", "\n", "In class we derived that for Logistic Regression the Negative Log Likelihood (NLL)\n", "that we needed to find the minizing parameters for is defined as \n", "$$\n", "NLL(w) = - \\sum_{i=1}^n y_i \\ln (\\sigma (w^\\intercal x)) + (1-y_i) \\ln (1 -\\sigma (w^\\intercal x))\n", "$$\n", "where $\\sigma(z) = \\frac{1}{1+e^{-z}}$ is the sigmoid function with derivative $\\frac{\\partial \\sigma}{\\partial z} = \\sigma(z)(1-\\sigma(z))$\n", "\n", "\n", "We need to prove that NLL(w) is a convex function (data X, y fixed as usual).\n", "A sum of convex functions is convex so we can ignore the sum and focus on just one element.\n", "i.e. we need to show that\n", "$$\n", "f(w) = - y \\ln (\\sigma (w^\\intercal x)) - (1-y) \\ln (1 -\\sigma (w^\\intercal x))\n", "$$\n", "is a convex function.\n", "\n", "\n", "\n", "We will do this in simple steps. First let us assume that x and w are 1D vectors i.e. numbers.\n", "To prove that $f$ is convex we can prove that $f''(w) \\geq 0$ for all $w$.\n", "* Step 1. Prove that $f'(w) = - y \\cdot x(1-\\sigma(w x)) - (1-y) \\cdot x \\cdot \\sigma(w x) = - (y - \\sigma(w x))x$ # check signs\n", "* Step 2. Prove that $f''(w) =  x \\cdot x  (\\sigma(w x) \\cdot (1 - \\sigma (w x)) $\n", "* Step 3. Argue that $f''(w) \\geq 0$ for all w\n", "\n", "\n", "To generalize this to d-dimensional $w$ and $x$ the same steps apply except now we have to do vector derivatives. \n", "\n", "* Step 1. Show that the matrix of derivatives of f is  $(y - \\sigma(w^\\intercal x)) x^\\intercal$\n", "\n", "The Hessian matrix is the second order matrix of derivatives.\n", "In the following we let $p=\\sigma(w^\\intercal x)$ so ease up the notation. \n", "\n", "* Step 2. Show that the Hessian of f is $p (1-p) x x^\\intercal$ (note that this is an outer product)   **hint: look at $(\\frac{\\partial f}{\\partial w_i})$**\n", "\n", "* Step 3. Show that $p (1-p) x x^\\intercal $ is a Positive Semidefinite Matrix\n", "\n", "\n", "\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}], "metadata": {"anaconda-cloud": {}, "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.4"}}, "nbformat": 4, "nbformat_minor": 2}