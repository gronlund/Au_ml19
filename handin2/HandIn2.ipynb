{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Nets for Multiclass Classification\n",
    "In this exercise you must implement a multi-class classifier based on neural nets.\n",
    "\n",
    "You must implement a one hidden layer neural net with relu activation function for the hidden layer and $K$ outputs that are not transformed (i.e.) the output neurons compute the identity function.\n",
    "\n",
    "As the error function, the $K$ outputs of the neural net are transformed by softmax and the loss is mean negative log likelihood (also considered in Hand In 1) and is explained in the next cell.\n",
    "\n",
    "For combatting overfitting you must implement weight decay and early stopping.\n",
    "\n",
    "With weight decay the cost of one data point becomes becomes\n",
    "\n",
    "$$\n",
    "-\\sum_{i=1}^k y_i \\ln \\textrm{softmax}(\\textrm{nn}(x))_i + \\lambda \\left(\\sum_{i,j} W_1[i,j]^2 + \\sum_{i, j} W_2[i, j]^2\\right)\n",
    "$$\n",
    "\n",
    "where $y$ is the one-in-k encoding of the data point label (one entry with a one, the rest are zero), nn(x) is the output of the neural net on data point $x$, $\\textrm{softmax}(\\textrm{nn}(x))_i$ is the $i$'th entry in the vector of *probabilities* computed by softmax, and $W_1$, and $W_2$ are the weight matrices for the neural net.\n",
    "\n",
    "For early stopping you must evaluate the mean nll loss and accuracy on a validation set after each epoch. For this computation, you must set the weight decay parameter to zero. \n",
    "\n",
    "\n",
    "## Your task is\n",
    "- Derive a very useful derivative described in the next cell\n",
    "- Complete the neural net class in net_classifier.py.\n",
    "- Test your implementation using net_test.py\n",
    "\n",
    "The prerequisites for the report is listed last.\n",
    "\n",
    "**Solve the derivative exercise in the next cell (derivation must be included in report) before you start the implementation shortly described in the following cell.**\n",
    "\n",
    "\n",
    "## Backpropagation Help\n",
    "As disucussed in class a nice way to understand backpropagation is given  [here](http://cs231n.github.io/optimization-2/)\n",
    "Everything can be coded in a vectorized way using matrices and vectors and their products. But this is **not** required.\n",
    "For example it is fine to implement the derivative computation assuming just one input point and then repeating that computation for each input data point in the mini-batch.\n",
    "\n",
    "The test in net_classifier.py tests the gradient of your cost using a numerical estimate of the gradient of the cost for every parameter (so both have to valid).\n",
    "\n",
    "**Start Early, Use The Study Cafe and the Discussion Board, Check Your Shapes/Dimensions, know there may be numerial issues**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivatives of Negative Log Likelihood as a function of the input to Softmax (the output of the neural net)\n",
    "In this exercise you must compute the derivative of the softmax operation and the negative log likelihood error in one step as this is much simpler than computing them step by step and  combining them with the chain rule. \n",
    "\n",
    "Let $z$ be the $k$-dimensional input to softmax and let $y$ denote the one-hot encoded vector of labels for a given data point.\n",
    "As a function of $z$, the Negative Log Likelihood loss for $z$ when the true label is $j$ is\n",
    "$$\n",
    "L(z) =  - \\sum_{i=1}^k y_i \\ln (\\textrm{softmax}(z)_i) = - \\ln (\\textrm{softmax}(z)_j) \n",
    "$$\n",
    "\n",
    "As a function of $z$, the cost $L$ is a function that maps $K$ numbers to one. We are looking for the partial derivative of $L$ relative to every entry in $z$ (the matrix of derivatives)\n",
    "\n",
    "The derivatie $d_{\\textrm{cost}, z}$ is a $1 \\times K$ vector where\n",
    "$$\n",
    "d_{\\textrm{cost}, z}[i] = \\frac{\\partial L}{\\partial z_i}\n",
    "$$\n",
    "\n",
    "**Given a one-hot-label label vector $y$ with $y_j=1$, show that:**\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial z_i} = - \\delta_{i, j} + \\frac{1}{\\sum_{a=1}^k e^{z_a}} \\cdot e^{z_i} =  - \\delta_{i, j} + \\textrm{softmax}(z)_i\n",
    "$$\n",
    "where $\\delta_{i,j} =1$ if $i=j$ and zero otherwise.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Net Implementation\n",
    "You need to complete the following methods\n",
    "- predict - return class predictions for all data points given \n",
    "- score - return mean accuracy of model on given data  \n",
    "- cost_grad - compute average cross entropy cost and gradient of this cost for all parameters\n",
    "- fit - given data run mini batch stochastic gradient descent to learn a good set of parameters that fit the data well, uses a validation set to test current model after each epoch and stores the best weights found.\n",
    "\n",
    "We have included a cost/gradient checker you can run by running net_classifier.py\n",
    "This test is not exhaustive so make your own if need be.\n",
    "\n",
    "When you have completed the exercise **run net_test.py** to test your algorithm on the MNIST digits. You should be able to get above 96 percentage accuracy both in sample and on the test set (results will fluctuate due to randomness in initialization and mini-batches).\n",
    "The code exports plots for your report.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report\n",
    "Make a report with two sections\n",
    "\n",
    "- Add a section called \"Part I: Derivative\" that includes your derivation of the derivative of the loss function\n",
    "- Add a section called \"Part II: Implementation and test\" that includes the code snippet for your forward pass and for your backward pass - Explain if anything did not work.  Include the train and validation data plots of loss and accuracy generated by net_test.py and comment on anything that looks unusual.   \n",
    "\n",
    "## Uploading to Black Board\n",
    "Upload the file net_classifier.py \n",
    "\n",
    "Upload one pdf with the report to blackboard together with net_classifier.py file\n",
    "\n",
    "**Ensure you upload the pdf separately!**\n",
    "\n",
    "**Remeber to put your names and student ids inside the pdf report!**\n",
    "\n",
    "**The PDF should be at the most 5 pages!**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
