{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Week 7 Exercises\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Vectorized  derivatives for Neural Nets - single data point\n",
    "\n",
    "Let $x$ be a data point ($1 \\times d$) vector and $W_1$ a weight matrix of shape $d \\times h$ used in the neural net \n",
    "$$\n",
    "\\textrm{nn}(x) = (xW_1 + b_1)W_2 + b_2\n",
    "$$\n",
    "Let $e$ be the error function considered\n",
    "\n",
    "Furthermore, let\n",
    "$$\n",
    "s = x W_1, \\quad v = \\frac{\\partial e}{\\partial s} = \\left[\\frac{\\partial e}{\\partial s[a]}, a=1,\\dots, h\\right]\n",
    "$$ \n",
    "both $1 \\times h$ vectors\n",
    "\n",
    "We need to check that \n",
    "\n",
    "$$\n",
    "\\frac{\\partial e}{\\partial W_1} = x^\\intercal v\n",
    "$$\n",
    "as claimed in the lecture. However we do not want to compute $\\frac{\\partial s}{\\partial W_1}$ as this gives a $h \\times (d \\times h)$ tensor or  a $h \\times (d * h)$ matrix (flattened $W_1$)\n",
    "\n",
    "We will simply check the formula by deriving\n",
    "$$\n",
    "\\frac{\\partial e}{\\partial W_1[i, j]}\n",
    "$$\n",
    "a derivative of just one entry in $W_1$ and see that it fits (considering $e$ relative to just one weight in $W_1$, then it is a function from 1 variable to 1 number and similarly $\\frac{\\partial s}{\\partial W_1[i,j]}$  considers a function from 1 number to $h$ numbers).\n",
    "\n",
    "\n",
    "\n",
    "**TASK:**\n",
    "- Step 1: Derive $\\frac{\\partial s}{\\partial W_1[i,j]} = \\left[\\frac{\\partial s[a]}{\\partial W_1[i, j]}, a=1,\\dots, h\\right]$  - it is a $h \\times 1$ vector. **Hint:** it contains mostly zeros except at one important place.\n",
    "- Step 2: Use Chain Rule to show that $\\frac{\\partial e}{\\partial W_1[i, j]} = x_i v_j$ and conclude the formula fits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2:  Vectorized  derivatives for Neural Nets - batch of data\n",
    "With the solution from Exercise 1 in place, we can now try and optimize for the case where we replace the single data point $x$ with a mini-batch of data $X$. We use the same neural net function as in Exercise 1.\n",
    "\n",
    "Let $X$ be a mini-batch of data points ($n \\times d$) matrix and $W_1$ a weight matrix of shape $d \\times h$\n",
    "To evaluate all elements in the minibatch we get\n",
    "\n",
    "$$\n",
    "\\textrm{nn}(X) = (XW_1 + b_1)W_2 + b_2\n",
    "$$\n",
    "\n",
    "which outputs an $n \\times 1$ vector instead of just one number.\n",
    "\n",
    "The error function considered is the sum of errors over the data in the batch  \n",
    "$$\n",
    "\\sum_{a=1}^n e(y_a, \\textrm{nn}(x_a))\n",
    "$$ \n",
    "\n",
    "where $e$ is some error function as above, and $x_a$ is the $a'th$ row in X (as a $1 \\times d$ vector) and $y_a$ is the target value for $x_a$.\n",
    "\n",
    "Furthermore, let\n",
    "\n",
    "$$\n",
    "S = X W_1, \\quad V = \\frac{\\partial e}{\\partial S} \n",
    "$$ \n",
    "\n",
    "both $n \\times h$ matrices, having a row for every input point $x \\in X$. This is the same as above applied for the $n$ points instead of one and placed in the rows of matrices $S$ and $V$.\n",
    "\n",
    "We know that if $X$ (and hence $S, V$) only has one row then the result is given above.\n",
    "To get derivatives of the sum we must sum the derivative and hence\n",
    "$$\n",
    " \\sum_{a=1}^ n \\frac{\\partial e(y_a, \\textrm{nn}(x_a))}{\\partial W} = \\sum_{a=1}^n x_a^\\intercal v_a\n",
    "$$\n",
    "where $x_a$ is the $a$'th row of $X$ and $v_a$ is the $a$'th row of V.\n",
    "\n",
    "\n",
    "**Task:** \n",
    "Show how to rewrite this sum into one matrix product and argue correctness.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Convolution and Pooling\n",
    "In this exercise we will see if we can get a better grasp of convolution and pooling.\n",
    "Your job is to implement basic convolution and pooling.\n",
    "\n",
    "For this we need the python package pillow  which you need to install\n",
    "\n",
    "- The Convolution operator that takes a $d \\times d$ weight matrix and 1 channel image, and applies the convolution with the weight vector with the image.\n",
    "- The max pooling operator takes an input image and a max pooling size and returns the pooled output.\n",
    "    For simplicity we only consider 2 x 2 max pooling\n",
    "- Test your convolution implementation with the $3 \\times 3$ matrix with -1 everywhere exept the middle and 8 in the middle is a classic edge detector pattern.\n",
    "- Test your pooling implementation by applying a $2 \\times 2$ max pooling to the output of the convolution.\n",
    "\n",
    "We assume that we pad the input to ensure the output of the convolution is the same width and height as the input image.\n",
    "\n",
    "To compare your implementation we have supplied code that applies the convolution and the pooling operator from the neural net package in pytorch.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image #(pacakge name is pillow - i.e. pip3 install pillow )\n",
    "import os\n",
    "filename = 'lena512_gray.bmp'\n",
    "\n",
    "img = Image.open('lena512_gray.bmp')\n",
    "lena = np.array(img)\n",
    "print('image shape', lena.shape)\n",
    "plt.imshow(lena, cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "def conv2d(img, w):\n",
    "    \"\"\" Return the result of applying the convolution defined by w to img - \n",
    "    for simplicity assume that w is square\"\"\"\n",
    "    w_dim = w.shape[0]\n",
    "    pad = w_dim - 2\n",
    "    padded_img = np.pad(img, [pad, pad], 'constant', constant_values=0)    \n",
    "    out = np.zeros(img.shape)\n",
    "    ### YOUR CODE HERE\n",
    "    ### END CODE\n",
    "    return out\n",
    "\n",
    "def max_pool2d(img):\n",
    "    \"\"\" Return the result of applying the 2 x 2 max pooling operator to mig (halve the width and height of image)\"\"\"\n",
    "    out = np.zeros((int(img.shape[0]/2), int(img.shape[1]/2)))\n",
    "    ### YOUR CODE HERE\n",
    "    ### END CODE\n",
    "    return out\n",
    "\n",
    "conv_filter = np.array([[-1., -1., -1.], [-1., 8, -1.], [-1., -1., -1.]])\n",
    "convoluted_lena = conv2d(lena, conv_filter)\n",
    "pooled_lena = max_pool2d(convoluted_lena)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 16))\n",
    "axes[0].imshow(convoluted_lena, cmap='gray')\n",
    "axes[1].imshow(pooled_lena, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "torch_lena = torch.from_numpy(lena).view(1, 1, lena.shape[0], lena.shape[1]).double()\n",
    "print('image shape', torch_lena.shape)\n",
    "tv = torch.tensor([[-1., -1., -1.], [-1., 8, -1.], [-1., -1., -1.]])\n",
    "tv = tv.view(1, 1, 3, 3).double()\n",
    "torch_convoluted_lena = F.conv2d(torch_lena, tv, torch.tensor([0.], dtype=torch.double), 1, 1, 1, 1)\n",
    "print('convoluted_lena shape', torch_convoluted_lena.shape)\n",
    "print('conv diff norm', np.linalg.norm(torch_convoluted_lena.numpy().squeeze() - convoluted_lena))\n",
    "torch_pooled_lena = F.max_pool2d(torch_convoluted_lena, kernel_size=(2, 2))\n",
    "print('pool diff norm', np.linalg.norm(torch_pooled_lena.numpy().squeeze() - pooled_lena))\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 16))\n",
    "axes[0].imshow(torch_convoluted_lena.numpy().squeeze(), cmap='gray')\n",
    "axes[1].imshow(torch_pooled_lena.numpy().squeeze(), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4: How Many Parameters Does the VGG net use\n",
    "The VGG net is a well known neural net architecture for image recognition. \n",
    "\n",
    "**How many parameters does it use? Write down a formula and compute the number!**\n",
    "\n",
    "It has the following architecture [https://pytorch.org/docs/0.4.0/_modules/torchvision/models/vgg.html]\n",
    "\n",
    "It takes as input an image of size 224 x 224 x 3.\n",
    "\n",
    "First it computes 64, 3 x 3 x 3 convolutions i.e. makes 64 channels. Then it makes 64, 3 x 3 x 64 convolutionas on the output of the previous convolution i.e. it makes 64 new channels. This is followd by  a $2 \\times 2$ max pooling operation.\n",
    "\n",
    "The pattern continues as described below: A number is the number of new convolutions made on previous input and 'M' means max pooling.\n",
    "\n",
    "64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'\n",
    "\n",
    "All convolutions are 3x3 using padding of 1 of zeros around the input so the convolution output size has height and width the same as the input.\n",
    "All poolings are $2 \\times 2$ non-overlapping (stride=2)\n",
    "\n",
    "After the fully convolutional layers it used standard fully connected layers as\n",
    "- 512 * 7 * 7, 4096 (why 512 * 7 * 7?)\n",
    "- 4096, 4096,\n",
    "- 4096, 1000\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5:  Maximal Margin By Hand \n",
    "\n",
    "- Assume you are given two points $x_1=(1, 1)$ with class -1 and $x_2 = (11, 11)$ with class 1. What is the maximum separating hyperplane. E.g. what are vectors w and b that maximally separates these points? What are the support vectors? What is the margin? Can you find the parameters w, b the SVM wil find without trying to solve the SVM problem directly but instead thinking a little. Try visualizing the data set.\n",
    "- If we have three points in class -1 (-10, -10), (-5, 2), (1, 1) and four points in class 1 (20, 23), (15, 17), (12, 10), (10, 12). What is the maximal separating hyperplane (w,b)? Try visualizing the data set before you work to hard.\n",
    "What are the support vectors? What is the margin? You can run the code below the next exercise to get the answers from actually running the python sklearn SVM implementation on the data.\n",
    "- Write down the exact form of the SVM problem we need to solve if the input data is defined as the 7 points above.\n",
    "The convex quadratic program was defined as \n",
    "\n",
    "$\\min_{w, b} \\frac{1}{2}{||w||^2}$\n",
    "\n",
    "s.t. $y_i(w^\\intercal x_i + b) \\geq 1$\n",
    "\n",
    "You job is thus to write down the constraints.\n",
    "\n",
    "\n",
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6: Regularized SVM\n",
    "\n",
    "Consider a regularized SVM wrong side of the margin penalty C.\n",
    "\n",
    "$\\min_{w, b, \\xi} \\frac{1}{2}{||w||^2} + C \\sum_{i=1}^n \\xi_i$\n",
    "\n",
    "s.t. $y_i(w^\\intercal x_i + b) \\geq 1 - \\xi_i$\n",
    "\n",
    "and $\\xi_i \\geq 0$\n",
    "\n",
    "Assume you are given three points $x_1=(1, 1)$ with class -1 and $x_2=(3, 3), x_3 = (11, 11)$ with class 1.\n",
    "- Write down the exact form of the SVM problem we need to solve if the input data $D=\\{x_1, x_2, x_3\\}$.\n",
    "  Thus, your job is thus to write down the constraints.\n",
    "\n",
    "- What is the best cost you can get when using the hyperplane $w = [ 0.1, 0.1]$ and  $b: -1.2$. i.e. how can you pick $\\xi_1, \\xi_2, \\xi_3$ such that the constraints are satisfied while minimizing the cost with w and b  fixed.\n",
    "\n",
    "- Is there a general way given, $w$ and $b$ to compute the best $\\xi_i$ \n",
    "- Can you find a $w, b$ with a smaller cost for $C=1$\n",
    "  If you like you can use the svm code below to experiment.\n",
    "  \n",
    "\n",
    "\n",
    "Also be sure to see how to apply an SVM implementation on data using Sklearn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for running the example above\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm\n",
    "\n",
    "def plot_hyperplane(w, *args, **kwargs): \n",
    "    if w[1]==0 and w[2]==0:\n",
    "        raise ValueError('Invalid hyperplane')\n",
    "    xmin, xmax, ymin, ymax = plt.axis()\n",
    "    \n",
    "    if w[2]==0:\n",
    "        # Vertical line\n",
    "        x = np.array((1/w[1], 1/w[1]))\n",
    "        y = np.array((ymin, ymax))\n",
    "    else:\n",
    "        x = np.array((xmin, xmax))\n",
    "        y = (-w[0]-w[1]*x)/w[2]       \n",
    "    # plot the line\n",
    "    plt.plot(x, y, *args, **kwargs)\n",
    "\n",
    "def run_svm(X, Y, kernel='linear', **kwargs):\n",
    "    # fit the model\n",
    "    clf = svm.SVC(kernel=kernel, **kwargs)\n",
    "    clf.fit(X, Y)\n",
    "\n",
    "    # get the separating hyperplane\n",
    "    \n",
    "    print('Hyperplane found w: {0} b: {1}'.format(clf.coef_[0],clf.intercept_[0]))\n",
    "    margin = 1.0/np.linalg.norm(clf.coef_[0])\n",
    "    print('Margin 1/||w||: {0}'.format(margin))\n",
    "    hyp = np.r_[clf.intercept_,clf.coef_[0]]\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=Y,cmap=plt.cm.Paired)\n",
    "    print('Support Vectors:')    \n",
    "    print(clf.support_vectors_)\n",
    "    \n",
    "    plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1],s=80, facecolors='none')\n",
    "    plt.xlim(X.min()-1,X.max()+1)\n",
    "    plt.ylim(X.min()-1,X.max()+1)\n",
    "    plot_hyperplane(hyp,'k-')\n",
    "    \n",
    "    print('what is hyp',hyp)\n",
    "    tmp = hyp[0]\n",
    "    hyp[0] = tmp + 1\n",
    "    plot_hyperplane(hyp,'k--')\n",
    "    hyp[0] = tmp -1\n",
    "    plot_hyperplane(hyp,'k--')\n",
    "    hyp[0] = tmp\n",
    "    \n",
    "    #plt.axis('tight')\n",
    "    plt.show()\n",
    "\n",
    "X = np.array([[1, 1], [11, 11]])\n",
    "Y = np.array([0, 1])\n",
    "print('First Exercise From Above')\n",
    "run_svm(X, Y, C=1)\n",
    "plt.show()\n",
    "\n",
    "print('Second Exercise From Above')\n",
    "X = np.array([(-10, -10), (-5, 2), (1, 1),  (20, 23), (15, 17), (12, 10), (10, 12)])\n",
    "Y = np.array([0, 0, 0, 1, 1, 1, 1])\n",
    "run_svm(X, Y)\n",
    "plt.show()\n",
    "\n",
    "# print('Larger Separable Data set')\n",
    "# n = 50\n",
    "# X = 2*np.r_[2*np.random.randn(n, 2) - [4, 4], 2*np.random.randn(n, 2) + [5, 5]]\n",
    "# Y = [-1] * n + [1] * n\n",
    "# run_svm(X, Y)\n",
    "# plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
